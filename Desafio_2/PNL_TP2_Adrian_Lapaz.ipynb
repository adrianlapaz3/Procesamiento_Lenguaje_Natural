{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaeUKaUm0HT/xCn8OrwJsb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianlapaz3/Procesamiento_Lenguaje_Natural/blob/main/PNL_TP2_Adrian_Lapaz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ojU06NnMCT8h",
        "outputId": "76c76005-526b-4a60-ae6a-74505a20bf74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m147.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "bigframes 2.4.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting packaging (from tensorflow)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting setuptools (from tensorflow)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting six>=1.12.0 (from tensorflow)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow)\n",
            "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting rich (from keras>=3.5.0->tensorflow)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras>=3.5.0->tensorflow)\n",
            "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting optree (from keras>=3.5.0->tensorflow)\n",
            "  Downloading optree-0.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras>=3.5.0->tensorflow)\n",
            "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m212.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m247.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m218.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m276.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m239.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m241.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m218.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m200.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m151.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m210.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m170.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m181.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m246.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m212.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m265.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m226.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m205.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m185.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m223.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m281.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m217.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m284.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m216.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m184.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading optree-0.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (416 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m233.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m235.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m265.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m272.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, six, setuptools, pygments, protobuf, packaging, opt-einsum, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, optree, ml-dtypes, markdown-it-py, h5py, google-pasta, astunparse, tensorboard, rich, keras, tensorflow\n",
            "  Attempting uninstall: namex\n",
            "    Found existing installation: namex 0.1.0\n",
            "    Uninstalling namex-0.1.0:\n",
            "      Successfully uninstalled namex-0.1.0\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 18.1.1\n",
            "    Uninstalling libclang-18.1.1:\n",
            "      Successfully uninstalled libclang-18.1.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.2.10\n",
            "    Uninstalling flatbuffers-25.2.10:\n",
            "      Successfully uninstalled flatbuffers-25.2.10\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.45.1\n",
            "    Uninstalling wheel-0.45.1:\n",
            "      Successfully uninstalled wheel-0.45.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 3.1.0\n",
            "    Uninstalling termcolor-3.1.0:\n",
            "      Successfully uninstalled termcolor-3.1.0\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.9.0\n",
            "    Uninstalling setuptools-80.9.0:\n",
            "      Successfully uninstalled setuptools-80.9.0\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.19.1\n",
            "    Uninstalling Pygments-2.19.1:\n",
            "      Successfully uninstalled Pygments-2.19.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.8\n",
            "    Uninstalling Markdown-3.8:\n",
            "      Successfully uninstalled Markdown-3.8\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.71.0\n",
            "    Uninstalling grpcio-1.71.0:\n",
            "      Successfully uninstalled grpcio-1.71.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.2\n",
            "    Uninstalling charset-normalizer-3.4.2:\n",
            "      Successfully uninstalled charset-normalizer-3.4.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 2.3.0\n",
            "    Uninstalling absl-py-2.3.0:\n",
            "      Successfully uninstalled absl-py-2.3.0\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: optree\n",
            "    Found existing installation: optree 0.16.0\n",
            "    Uninstalling optree-0.16.0:\n",
            "      Successfully uninstalled optree-0.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.13.0\n",
            "    Uninstalling h5py-3.13.0:\n",
            "      Successfully uninstalled h5py-3.13.0\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: astunparse\n",
            "    Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 14.0.0\n",
            "    Uninstalling rich-14.0.0:\n",
            "      Successfully uninstalled rich-14.0.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "langchain-core 0.3.60 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "bigframes 2.4.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.0 astunparse-1.6.3 certifi-2025.4.26 charset-normalizer-3.4.2 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 idna-3.10 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.1.3 opt-einsum-3.4.0 optree-0.16.0 packaging-25.0 protobuf-5.29.5 pygments-2.19.1 requests-2.32.3 rich-14.0.0 setuptools-80.9.0 six-1.17.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 typing-extensions-4.13.2 urllib3-2.4.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "certifi",
                  "pkg_resources",
                  "six"
                ]
              },
              "id": "e1024a6438e949d79625ed1839839e91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras\n",
            "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting absl-py (from keras)\n",
            "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting numpy (from keras)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich (from keras)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting h5py (from keras)\n",
            "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
            "Collecting ml-dtypes (from keras)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting packaging (from keras)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting typing-extensions>=4.6.0 (from optree->keras)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras)\n",
            "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m174.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m201.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading optree-0.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (416 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m154.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m191.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m165.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m205.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m146.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: namex, typing-extensions, pygments, packaging, numpy, mdurl, absl-py, optree, ml-dtypes, markdown-it-py, h5py, rich, keras\n",
            "  Attempting uninstall: namex\n",
            "    Found existing installation: namex 0.1.0\n",
            "    Uninstalling namex-0.1.0:\n",
            "      Successfully uninstalled namex-0.1.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.19.1\n",
            "    Uninstalling Pygments-2.19.1:\n",
            "      Successfully uninstalled Pygments-2.19.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 2.3.0\n",
            "    Uninstalling absl-py-2.3.0:\n",
            "      Successfully uninstalled absl-py-2.3.0\n",
            "  Attempting uninstall: optree\n",
            "    Found existing installation: optree 0.16.0\n",
            "    Uninstalling optree-0.16.0:\n",
            "      Successfully uninstalled optree-0.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.1\n",
            "    Uninstalling ml_dtypes-0.5.1:\n",
            "      Successfully uninstalled ml_dtypes-0.5.1\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.13.0\n",
            "    Uninstalling h5py-3.13.0:\n",
            "      Successfully uninstalled h5py-3.13.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 14.0.0\n",
            "    Uninstalling rich-14.0.0:\n",
            "      Successfully uninstalled rich-14.0.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "langchain-core 0.3.60 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "bigframes 2.4.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-2.3.0 h5py-3.13.0 keras-3.10.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.2.6 optree-0.16.0 packaging-25.0 pygments-2.19.1 rich-14.0.0 typing-extensions-4.13.2\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m203.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m180.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m167.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m146.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-cache-dir numpy\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir tensorflow\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir keras\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f-w8vsGZES5D",
        "outputId": "f2b8eaa0-96b2-44e3-8b62-6413057d6329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~cipy (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~cipy (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: scipy\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~cipy (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import multiprocessing\n",
        "import logging\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "import sklearn\n",
        "from packaging.version import parse as parse_version\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# --- Enable Gensim Logging ---\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# --- NLTK Resource Download Function ---\n",
        "def download_nltk_resource(resource_path, resource_name):\n",
        "    try:\n",
        "        nltk.data.find(resource_path)\n",
        "        logging.info(f\"NLTK resource '{resource_name}' ({resource_path}) already downloaded.\")\n",
        "    except LookupError:\n",
        "        logging.info(f\"NLTK resource '{resource_name}' ({resource_path}) not found. Downloading...\")\n",
        "        nltk.download(resource_name, quiet=False)\n",
        "        logging.info(f\"NLTK resource '{resource_name}' downloaded.\")\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "download_nltk_resource('corpora/gutenberg', 'gutenberg')\n",
        "download_nltk_resource('tokenizers/punkt', 'punkt')\n",
        "download_nltk_resource('corpora/stopwords', 'stopwords')\n",
        "download_nltk_resource('corpora/wordnet', 'wordnet')\n",
        "download_nltk_resource('corpora/omw-1.4', 'omw-1.4')\n",
        "print(\"NLTK resource checks complete.\")\n",
        "print(\"Setup and imports complete.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leSnYEBkCsii",
        "outputId": "46073fec-51be-4c23-d08e-443b0f77c436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK resource checks complete.\n",
            "Setup and imports complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPzmNMXQE0wb",
        "outputId": "6fd2b881-57ad-48fe-d6ff-35cc48a1db42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# \"Hamlet\" Corpus\n",
        "# ==============================================================================\n",
        "corpus_id = 'shakespeare-hamlet.txt'\n",
        "try:\n",
        "    raw_hamlet_text = gutenberg.raw(corpus_id)\n",
        "    logging.info(f\"Successfully loaded raw text for '{corpus_id}'. Length: {len(raw_hamlet_text)} characters.\")\n",
        "except LookupError:\n",
        "    logging.error(f\"Corpus '{corpus_id}' not found in NLTK Gutenberg. Please ensure it's downloaded.\")\n",
        "    raw_hamlet_text = \"\"\n",
        "\n",
        "# Isolate actual dialogue. This is iterative and may need further refinement.\n",
        "# Words to specifically track during preprocessing for debugging\n",
        "debug_words_to_trace = {\"love\", \"revenge\", \"madness\", \"skull\"}\n",
        "\n",
        "processed_lines_for_tokenization = []\n",
        "if raw_hamlet_text:\n",
        "    start_marker = \"[The Tragedie of Hamlet by William Shakespeare\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\"\n",
        "\n",
        "    try:\n",
        "        start_index = raw_hamlet_text.lower().find(start_marker.lower())\n",
        "        actual_text_start = 0\n",
        "        if start_index != -1:\n",
        "            end_of_header_line_index = raw_hamlet_text.find('\\n', start_index + len(start_marker))\n",
        "            if end_of_header_line_index != -1:\n",
        "                 actual_text_start = end_of_header_line_index + 1\n",
        "            else:\n",
        "                 actual_text_start = start_index + len(start_marker)\n",
        "            raw_hamlet_text = raw_hamlet_text[actual_text_start:]\n",
        "            logging.info(f\"Attempted to remove Gutenberg header. New start of text preview: {raw_hamlet_text[:200]}\")\n",
        "\n",
        "        end_index = raw_hamlet_text.lower().find(end_marker.lower())\n",
        "        if end_index != -1:\n",
        "            raw_hamlet_text = raw_hamlet_text[:end_index]\n",
        "            logging.info(\"Attempted to remove Gutenberg footer.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during header/footer removal: {e}\")\n",
        "\n",
        "    lines = raw_hamlet_text.splitlines()\n",
        "\n",
        "    act_scene_pattern = r\"^\\s*(ACTUS|ACT|SCENE|SCOENA)\\s+[IVXLCDM]+\\.?\\s*$\"\n",
        "    speaker_pattern_advanced = r\"^([A-Z][A-Z\\s\\.]{0,25}\\.)\\s*\"\n",
        "    stage_direction_bracket_pattern = r\"\\[.*?\\]\"\n",
        "    stage_direction_paren_pattern = r\"\\(.*?\\)\"\n",
        "\n",
        "    for line_number, line_content_original in enumerate(lines):\n",
        "        line = line_content_original.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Debug: Check if line contains any debug words BEFORE any filtering\n",
        "        for dw in debug_words_to_trace:\n",
        "            if dw in line.lower():\n",
        "                logging.debug(f\"Line {line_number+1} RAW: Contains '{dw}'. Line: '{line_content_original}'\")\n",
        "\n",
        "        if re.fullmatch(act_scene_pattern, line, re.IGNORECASE):\n",
        "            logging.debug(f\"Line {line_number+1}: Filtered Act/Scene: {line_content_original}\")\n",
        "            continue\n",
        "\n",
        "        # Remove bracketed and parenthesized stage directions first\n",
        "        line_no_brackets = re.sub(stage_direction_bracket_pattern, \"\", line)\n",
        "        line_no_parens = re.sub(stage_direction_paren_pattern, \"\", line_no_brackets)\n",
        "        line = line_no_parens.strip()\n",
        "\n",
        "        # Attempt to remove speaker tags\n",
        "        cleaned_after_speaker_removal = re.sub(speaker_pattern_advanced, \"\", line)\n",
        "        if len(cleaned_after_speaker_removal) < len(line):\n",
        "            logging.debug(f\"Line {line_number+1}: Speaker pattern matched. Original segment: '{line}', After speaker removal: '{cleaned_after_speaker_removal}'\")\n",
        "            line = cleaned_after_speaker_removal.strip()\n",
        "\n",
        "        if not line:\n",
        "            logging.debug(f\"Line {line_number+1}: Became empty after cleaning stage directions/speaker. Original: '{line_content_original}'\")\n",
        "            continue\n",
        "\n",
        "        # Filter lines that are likely just remaining stage directions or very short non-dialogue\n",
        "        if (line.isupper() and len(line.split()) < 5) or len(line.split()) < 2: # Require at least 2 words for context\n",
        "            logging.debug(f\"Line {line_number+1}: Filtered short/uppercase line: '{line}' from original: '{line_content_original}'\")\n",
        "            continue\n",
        "\n",
        "        processed_lines_for_tokenization.append(line)\n",
        "\n",
        "logging.info(f\"Number of lines after structural filtering: {len(processed_lines_for_tokenization)}\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_lemmatize_remove_stopwords(text_line, line_idx_for_debug=\"Unknown\"):\n",
        "    # Tokenize first\n",
        "    tokens_raw = word_tokenize(text_line.lower())\n",
        "\n",
        "    # Debug: Check if any of the debug_words are present after initial tokenization\n",
        "    for dw in debug_words_to_trace:\n",
        "        if dw in tokens_raw:\n",
        "            logging.debug(f\"Line {line_idx_for_debug} DEBUG_TRACE (Post-Tokenize): Word '{dw}' found. Tokens: {tokens_raw}\")\n",
        "\n",
        "    # Keep only alphabetic tokens\n",
        "    alpha_tokens = [word for word in tokens_raw if word.isalpha()]\n",
        "    for dw in debug_words_to_trace:\n",
        "        if dw not in alpha_tokens and dw in tokens_raw:\n",
        "             logging.debug(f\"Line {line_idx_for_debug} DEBUG_TRACE (Post-IsAlpha): Word '{dw}' REMOVED by isalpha. Original tokens: {tokens_raw}, Alpha: {alpha_tokens}\")\n",
        "        elif dw in alpha_tokens:\n",
        "             logging.debug(f\"Line {line_idx_for_debug} DEBUG_TRACE (Post-IsAlpha): Word '{dw}' KEPT after isalpha. Alpha: {alpha_tokens}\")\n",
        "\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in alpha_tokens]\n",
        "    for dw in debug_words_to_trace:\n",
        "        lem_dw = lemmatizer.lemmatize(dw)\n",
        "        if lem_dw not in lemmatized_tokens and dw in alpha_tokens:\n",
        "            if dw != lem_dw :\n",
        "                 logging.debug(f\"Line {line_idx_for_debug} DEBUG_TRACE (Post-Lemma): Word '{dw}' changed to '{lem_dw}'. Lemmatized: {lemmatized_tokens}\")\n",
        "\n",
        "        elif lem_dw in lemmatized_tokens:\n",
        "             logging.debug(f\"Line {line_idx_for_debug} DEBUG_TRACE (Post-Lemma): Word '{dw}' (as '{lem_dw}') KEPT. Lemmatized: {lemmatized_tokens}\")\n",
        "\n",
        "\n",
        "    # Remove stopwords and short words (length 1)\n",
        "    final_processed_tokens = [word for word in lemmatized_tokens if word not in english_stopwords and len(word) > 1]\n",
        "    for dw in debug_words_to_trace:\n",
        "        lem_dw = lemmatizer.lemmatize(dw)\n",
        "        if lem_dw not in final_processed_tokens and lem_dw in lemmatized_tokens:\n",
        "            reason = \"stopword\" if lem_dw in english_stopwords else \"length<=1\" if len(lem_dw) <=1 else \"unknown\"\n",
        "            logging.debug(f\"Line {line_idx_for_debug} DEBUG_TRACE (Post-Stopword/Len): Word '{dw}' (as '{lem_dw}') REMOVED. Reason: {reason}. Lemmatized before filter: {lemmatized_tokens}\")\n",
        "        elif lem_dw in final_processed_tokens:\n",
        "            logging.debug(f\"Line {line_idx_for_debug} DEBUG_TRACE (Post-Stopword/Len): Word '{dw}' (as '{lem_dw}') KEPT. Final: {final_processed_tokens}\")\n",
        "\n",
        "    return final_processed_tokens\n",
        "\n",
        "sentences_for_word2vec = []\n",
        "if processed_lines_for_tokenization:\n",
        "    for idx, line_text in enumerate(processed_lines_for_tokenization):\n",
        "        processed_tokens = tokenize_lemmatize_remove_stopwords(line_text, line_idx_for_debug=str(idx+1)) # Pass index\n",
        "        if processed_tokens and len(processed_tokens) > 1:\n",
        "            sentences_for_word2vec.append(processed_tokens)\n",
        "\n",
        "if sentences_for_word2vec:\n",
        "    print(f\"Preprocessing complete. Number of effectively processed sentences for Word2Vec: {len(sentences_for_word2vec)}\")\n",
        "    print(\"\\nExample of first 5 processed sentences (post-lemmatization & stopword removal):\")\n",
        "    for i in range(min(5, len(sentences_for_word2vec))):\n",
        "        print(sentences_for_word2vec[i])\n",
        "else:\n",
        "    print(\"No sentences available after full preprocessing. Review filtering, corpus content, and Gensim INFO logs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2c_M3AJCse7",
        "outputId": "e7e26abb-6c32-46f1-e28e-b37508fb8075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete. Number of effectively processed sentences for Word2Vec: 3735\n",
            "\n",
            "Example of first 5 processed sentences (post-lemmatization & stopword removal):\n",
            "['actus', 'primus', 'scoena', 'prima']\n",
            "['enter', 'barnardo', 'francisco', 'two', 'centinels']\n",
            "['fran', 'nay', 'answer', 'stand', 'vnfold']\n",
            "['bar', 'long', 'liue', 'king']\n",
            "['fran', 'barnardo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El preprocesamiento del texto de Hamlet encontró 3735 oraciones efectivas que es una cantidad aceptable para un modelo Word2Vec. Los Tokens incluyen:\n",
        "* Indicadores estructurales de la obra (ej., `['actus', 'primus', 'scoena', 'prima']`) para que el modelo aprenda el contexto de éstos.\n",
        "* Direcciones de escena y nombres de personajes (ej., `['enter', 'barnardo', 'francisco']`, `['fran', 'nay', 'answer']`) que son utiles para analizar interacciones de personajes.\n",
        "* Formas antiguas de escribir palabras (ej., `vnfold`, `liue`) por ende, el modelo aprenderá embeddings para estas formas específicas.\n",
        "* Buena base para Word2Vec ya que se obtuvo una lematización (\"raíces de palabras\") y eliminación de palabras vacias (ej. \"the\") mejoran la calidad de los tokens para el entrenamiento.\n"
      ],
      "metadata": {
        "id": "a-B2JvVlHc1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Train Gensim Word2Vec Model\n",
        "# ==============================================================================\n",
        "\n",
        "class LossCallback(CallbackAny2Vec):\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "        self.cumulative_loss_at_previous_epoch = 0.0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        current_cumulative_loss = model.get_latest_training_loss()\n",
        "        loss_this_epoch = current_cumulative_loss - self.cumulative_loss_at_previous_epoch\n",
        "        logging.info(f\"Epoch {self.epoch}: Cumulative loss = {current_cumulative_loss:.4f}, Loss this epoch = {loss_this_epoch:.4f}\")\n",
        "        print(f\"Epoch {self.epoch}: Cumulative loss = {current_cumulative_loss:.4f}, Loss this epoch = {loss_this_epoch:.4f}\")\n",
        "        self.cumulative_loss_at_previous_epoch = current_cumulative_loss\n",
        "        self.epoch += 1\n",
        "\n",
        "w2v_model_hamlet = None\n",
        "wv_hamlet = None\n",
        "\n",
        "MIN_SENTENCES_FOR_TRAINING = 50\n",
        "if sentences_for_word2vec and len(sentences_for_word2vec) >= MIN_SENTENCES_FOR_TRAINING:\n",
        "    print(f\"Starting Word2Vec model training with {len(sentences_for_word2vec)} sentences...\")\n",
        "\n",
        "    w2v_model_hamlet = Word2Vec(\n",
        "        sentences=sentences_for_word2vec,\n",
        "        vector_size=100,\n",
        "        window=5,\n",
        "        min_count=1,\n",
        "        sg=1,\n",
        "        hs=0,\n",
        "        negative=10,\n",
        "        workers=max(1, multiprocessing.cpu_count() - 1),\n",
        "        epochs=100,\n",
        "        compute_loss=True,\n",
        "        callbacks=[LossCallback()],\n",
        "        alpha=0.025,\n",
        "        min_alpha=0.0001\n",
        "    )\n",
        "\n",
        "    wv_hamlet = w2v_model_hamlet.wv\n",
        "    print(f\"\\nWord2Vec model training complete.\")\n",
        "    print(f\"New Vocabulary size (with min_count=1): {len(wv_hamlet.index_to_key)}\")\n",
        "else:\n",
        "    print(f\"Skipping Word2Vec model training: Not enough processed sentences available (found {len(sentences_for_word2vec) if sentences_for_word2vec else 0}, need at least {MIN_SENTENCES_FOR_TRAINING}).\")\n",
        "    logging.warning(f\"Word2Vec training skipped. Number of processed sentences: {len(sentences_for_word2vec) if sentences_for_word2vec else 0}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr5kw-iACsbi",
        "outputId": "de956df9-0e89-452b-dfd8-f59b18aa1243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Word2Vec model training with 3735 sentences...\n",
            "Epoch 0: Cumulative loss = 274219.2500, Loss this epoch = 274219.2500\n",
            "Epoch 1: Cumulative loss = 531738.9375, Loss this epoch = 257519.6875\n",
            "Epoch 2: Cumulative loss = 725770.8125, Loss this epoch = 194031.8750\n",
            "Epoch 3: Cumulative loss = 883290.1250, Loss this epoch = 157519.3125\n",
            "Epoch 4: Cumulative loss = 1023993.8750, Loss this epoch = 140703.7500\n",
            "Epoch 5: Cumulative loss = 1157890.2500, Loss this epoch = 133896.3750\n",
            "Epoch 6: Cumulative loss = 1287612.1250, Loss this epoch = 129721.8750\n",
            "Epoch 7: Cumulative loss = 1414280.8750, Loss this epoch = 126668.7500\n",
            "Epoch 8: Cumulative loss = 1540271.0000, Loss this epoch = 125990.1250\n",
            "Epoch 9: Cumulative loss = 1663991.5000, Loss this epoch = 123720.5000\n",
            "Epoch 10: Cumulative loss = 1787845.2500, Loss this epoch = 123853.7500\n",
            "Epoch 11: Cumulative loss = 1909797.2500, Loss this epoch = 121952.0000\n",
            "Epoch 12: Cumulative loss = 2030220.8750, Loss this epoch = 120423.6250\n",
            "Epoch 13: Cumulative loss = 2142843.2500, Loss this epoch = 112622.3750\n",
            "Epoch 14: Cumulative loss = 2243317.0000, Loss this epoch = 100473.7500\n",
            "Epoch 15: Cumulative loss = 2342113.0000, Loss this epoch = 98796.0000\n",
            "Epoch 16: Cumulative loss = 2440293.2500, Loss this epoch = 98180.2500\n",
            "Epoch 17: Cumulative loss = 2536626.7500, Loss this epoch = 96333.5000\n",
            "Epoch 18: Cumulative loss = 2631365.5000, Loss this epoch = 94738.7500\n",
            "Epoch 19: Cumulative loss = 2724055.5000, Loss this epoch = 92690.0000\n",
            "Epoch 20: Cumulative loss = 2815338.0000, Loss this epoch = 91282.5000\n",
            "Epoch 21: Cumulative loss = 2904686.2500, Loss this epoch = 89348.2500\n",
            "Epoch 22: Cumulative loss = 2990951.2500, Loss this epoch = 86265.0000\n",
            "Epoch 23: Cumulative loss = 3076566.5000, Loss this epoch = 85615.2500\n",
            "Epoch 24: Cumulative loss = 3159963.0000, Loss this epoch = 83396.5000\n",
            "Epoch 25: Cumulative loss = 3241154.5000, Loss this epoch = 81191.5000\n",
            "Epoch 26: Cumulative loss = 3320550.5000, Loss this epoch = 79396.0000\n",
            "Epoch 27: Cumulative loss = 3397235.7500, Loss this epoch = 76685.2500\n",
            "Epoch 28: Cumulative loss = 3471705.7500, Loss this epoch = 74470.0000\n",
            "Epoch 29: Cumulative loss = 3544029.5000, Loss this epoch = 72323.7500\n",
            "Epoch 30: Cumulative loss = 3614149.5000, Loss this epoch = 70120.0000\n",
            "Epoch 31: Cumulative loss = 3682352.7500, Loss this epoch = 68203.2500\n",
            "Epoch 32: Cumulative loss = 3748670.0000, Loss this epoch = 66317.2500\n",
            "Epoch 33: Cumulative loss = 3812398.7500, Loss this epoch = 63728.7500\n",
            "Epoch 34: Cumulative loss = 3874088.7500, Loss this epoch = 61690.0000\n",
            "Epoch 35: Cumulative loss = 3935369.2500, Loss this epoch = 61280.5000\n",
            "Epoch 36: Cumulative loss = 3993605.2500, Loss this epoch = 58236.0000\n",
            "Epoch 37: Cumulative loss = 4050715.5000, Loss this epoch = 57110.2500\n",
            "Epoch 38: Cumulative loss = 4106329.0000, Loss this epoch = 55613.5000\n",
            "Epoch 39: Cumulative loss = 4160859.7500, Loss this epoch = 54530.7500\n",
            "Epoch 40: Cumulative loss = 4211400.0000, Loss this epoch = 50540.2500\n",
            "Epoch 41: Cumulative loss = 4256735.5000, Loss this epoch = 45335.5000\n",
            "Epoch 42: Cumulative loss = 4301288.0000, Loss this epoch = 44552.5000\n",
            "Epoch 43: Cumulative loss = 4345652.0000, Loss this epoch = 44364.0000\n",
            "Epoch 44: Cumulative loss = 4388845.5000, Loss this epoch = 43193.5000\n",
            "Epoch 45: Cumulative loss = 4431059.5000, Loss this epoch = 42214.0000\n",
            "Epoch 46: Cumulative loss = 4471998.5000, Loss this epoch = 40939.0000\n",
            "Epoch 47: Cumulative loss = 4513183.0000, Loss this epoch = 41184.5000\n",
            "Epoch 48: Cumulative loss = 4552868.5000, Loss this epoch = 39685.5000\n",
            "Epoch 49: Cumulative loss = 4592003.0000, Loss this epoch = 39134.5000\n",
            "Epoch 50: Cumulative loss = 4630561.5000, Loss this epoch = 38558.5000\n",
            "Epoch 51: Cumulative loss = 4668455.0000, Loss this epoch = 37893.5000\n",
            "Epoch 52: Cumulative loss = 4705956.5000, Loss this epoch = 37501.5000\n",
            "Epoch 53: Cumulative loss = 4742907.5000, Loss this epoch = 36951.0000\n",
            "Epoch 54: Cumulative loss = 4779352.0000, Loss this epoch = 36444.5000\n",
            "Epoch 55: Cumulative loss = 4814724.5000, Loss this epoch = 35372.5000\n",
            "Epoch 56: Cumulative loss = 4849727.0000, Loss this epoch = 35002.5000\n",
            "Epoch 57: Cumulative loss = 4884862.0000, Loss this epoch = 35135.0000\n",
            "Epoch 58: Cumulative loss = 4919747.0000, Loss this epoch = 34885.0000\n",
            "Epoch 59: Cumulative loss = 4953966.0000, Loss this epoch = 34219.0000\n",
            "Epoch 60: Cumulative loss = 4988155.5000, Loss this epoch = 34189.5000\n",
            "Epoch 61: Cumulative loss = 5021536.0000, Loss this epoch = 33380.5000\n",
            "Epoch 62: Cumulative loss = 5055337.0000, Loss this epoch = 33801.0000\n",
            "Epoch 63: Cumulative loss = 5088359.5000, Loss this epoch = 33022.5000\n",
            "Epoch 64: Cumulative loss = 5121055.5000, Loss this epoch = 32696.0000\n",
            "Epoch 65: Cumulative loss = 5153511.5000, Loss this epoch = 32456.0000\n",
            "Epoch 66: Cumulative loss = 5186009.5000, Loss this epoch = 32498.0000\n",
            "Epoch 67: Cumulative loss = 5217650.5000, Loss this epoch = 31641.0000\n",
            "Epoch 68: Cumulative loss = 5249553.5000, Loss this epoch = 31903.0000\n",
            "Epoch 69: Cumulative loss = 5281353.0000, Loss this epoch = 31799.5000\n",
            "Epoch 70: Cumulative loss = 5312603.0000, Loss this epoch = 31250.0000\n",
            "Epoch 71: Cumulative loss = 5343747.0000, Loss this epoch = 31144.0000\n",
            "Epoch 72: Cumulative loss = 5374536.0000, Loss this epoch = 30789.0000\n",
            "Epoch 73: Cumulative loss = 5405686.5000, Loss this epoch = 31150.5000\n",
            "Epoch 74: Cumulative loss = 5436035.0000, Loss this epoch = 30348.5000\n",
            "Epoch 75: Cumulative loss = 5465994.0000, Loss this epoch = 29959.0000\n",
            "Epoch 76: Cumulative loss = 5496386.5000, Loss this epoch = 30392.5000\n",
            "Epoch 77: Cumulative loss = 5526802.0000, Loss this epoch = 30415.5000\n",
            "Epoch 78: Cumulative loss = 5556815.5000, Loss this epoch = 30013.5000\n",
            "Epoch 79: Cumulative loss = 5586709.5000, Loss this epoch = 29894.0000\n",
            "Epoch 80: Cumulative loss = 5616634.5000, Loss this epoch = 29925.0000\n",
            "Epoch 81: Cumulative loss = 5645868.0000, Loss this epoch = 29233.5000\n",
            "Epoch 82: Cumulative loss = 5675384.0000, Loss this epoch = 29516.0000\n",
            "Epoch 83: Cumulative loss = 5704532.0000, Loss this epoch = 29148.0000\n",
            "Epoch 84: Cumulative loss = 5733754.5000, Loss this epoch = 29222.5000\n",
            "Epoch 85: Cumulative loss = 5763162.0000, Loss this epoch = 29407.5000\n",
            "Epoch 86: Cumulative loss = 5792191.0000, Loss this epoch = 29029.0000\n",
            "Epoch 87: Cumulative loss = 5821418.0000, Loss this epoch = 29227.0000\n",
            "Epoch 88: Cumulative loss = 5850195.5000, Loss this epoch = 28777.5000\n",
            "Epoch 89: Cumulative loss = 5878896.5000, Loss this epoch = 28701.0000\n",
            "Epoch 90: Cumulative loss = 5908004.5000, Loss this epoch = 29108.0000\n",
            "Epoch 91: Cumulative loss = 5936472.5000, Loss this epoch = 28468.0000\n",
            "Epoch 92: Cumulative loss = 5965055.0000, Loss this epoch = 28582.5000\n",
            "Epoch 93: Cumulative loss = 5992502.0000, Loss this epoch = 27447.0000\n",
            "Epoch 94: Cumulative loss = 6021357.5000, Loss this epoch = 28855.5000\n",
            "Epoch 95: Cumulative loss = 6049626.5000, Loss this epoch = 28269.0000\n",
            "Epoch 96: Cumulative loss = 6078158.0000, Loss this epoch = 28531.5000\n",
            "Epoch 97: Cumulative loss = 6106383.5000, Loss this epoch = 28225.5000\n",
            "Epoch 98: Cumulative loss = 6134631.0000, Loss this epoch = 28247.5000\n",
            "Epoch 99: Cumulative loss = 6162858.0000, Loss this epoch = 28227.0000\n",
            "\n",
            "Word2Vec model training complete.\n",
            "New Vocabulary size (with min_count=1): 4145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El entrenamiento del modelo Word2Vec con 3735 oraciones procesadas de \"Hamlet\" a lo largo de 100 épocas demuestra un aprendizaje efectivo, evidenciado por la progresiva disminución del \"Loss this epoch\" (de 274k a 98k). Aunque la estabilización del loss hacia el final sugiere una convergencia adecuada para este corpus específico y de tamaño limitado, el uso de min_count=1 resultó en un vocabulario de 4145 palabras. Esto indica que los embeddings generados serán altamente especializados en el lenguaje y contexto de \"Hamlet\", si bien la representación vectorial de términos muy poco frecuentes podría ser menos robusta debido a su escaso contexto en los datos de entrenamiento."
      ],
      "metadata": {
        "id": "PUXHKNe-Nh5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Test Terms of Interest and Explain Similarities\n",
        "# ==============================================================================\n",
        "\n",
        "if wv_hamlet:\n",
        "    words_of_interest = [\"hamlet\", \"king\", \"queen\", \"ghost\", \"ophelia\", \"death\",\n",
        "                         \"play\", \"revenge\", \"love\", \"madness\", \"skull\", \"denmark\",\n",
        "                         \"polonius\", \"laertes\", \"horatio\", \"gertrude\", \"claudius\", \"fortinbras\",\n",
        "                         \"poison\", \"sword\", \"yorick\", \"doubt\", \"fate\", \"honour\"]\n",
        "\n",
        "    for term in words_of_interest:\n",
        "        if term in wv_hamlet:\n",
        "            try:\n",
        "                similar_words = wv_hamlet.most_similar(term, topn=5)\n",
        "                print(f\"\\nMost similar to '{term}':\")\n",
        "                for word, score in similar_words:\n",
        "                    print(f\"  {word}: {score:.4f}\")\n",
        "            except KeyError:\n",
        "                print(f\"\\n'{term}' was in vocab but KeyError during most_similar (unexpected).\")\n",
        "        else:\n",
        "            print(f\"\\n'{term}' not found in vocabulary (likely due to frequency or preprocessing).\")\n",
        "\n",
        "    pairs_to_compare = [\n",
        "        (\"hamlet\",\"ophelia\"), (\"king\",\"queen\"), (\"death\",\"ghost\"),\n",
        "        (\"play\",\"revenge\"), (\"hamlet\", \"madness\"), (\"king\", \"claudius\"),\n",
        "        (\"ophelia\", \"love\"), (\"laertes\", \"revenge\"), (\"poison\", \"death\")\n",
        "    ]\n",
        "    print(\"\\nPair similarities:\")\n",
        "    for w1, w2 in pairs_to_compare:\n",
        "        if w1 in wv_hamlet and w2 in wv_hamlet:\n",
        "            try:\n",
        "                similarity = wv_hamlet.similarity(w1, w2)\n",
        "                print(f\"  {w1} - {w2}: {similarity:.4f}\")\n",
        "            except KeyError:\n",
        "                 print(f\"  Could not calculate similarity for {w1}-{w2} (unexpected KeyError).\")\n",
        "        else:\n",
        "            print(f\"  Cannot compare '{w1}' and '{w2}': one or both not in vocabulary.\")\n",
        "else:\n",
        "    print(\"Word2Vec model (wv_hamlet) not available for similarity testing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLgce1KqOezl",
        "outputId": "fe7b62c5-8a8b-4842-eae4-0984815cc674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most similar to 'hamlet':\n",
            "  vnbrac: 0.5818\n",
            "  doublet: 0.5772\n",
            "  dar: 0.5572\n",
            "  combate: 0.5508\n",
            "  postscript: 0.5363\n",
            "\n",
            "Most similar to 'king':\n",
            "  discouery: 0.5048\n",
            "  authority: 0.4761\n",
            "  earnest: 0.4677\n",
            "  secricie: 0.4654\n",
            "  rosincrance: 0.4642\n",
            "\n",
            "Most similar to 'queen':\n",
            "  willow: 0.7905\n",
            "  aslant: 0.7713\n",
            "  inobled: 0.7246\n",
            "  nightly: 0.7182\n",
            "  vertuous: 0.6852\n",
            "\n",
            "Most similar to 'ghost':\n",
            "  gent: 0.6654\n",
            "  adulterate: 0.6547\n",
            "  seate: 0.6359\n",
            "  ambass: 0.6319\n",
            "  befitted: 0.5916\n",
            "\n",
            "Most similar to 'ophelia':\n",
            "  beautifed: 0.7816\n",
            "  idoll: 0.7765\n",
            "  sprung: 0.7581\n",
            "  interre: 0.7351\n",
            "  hugger: 0.7309\n",
            "\n",
            "Most similar to 'death':\n",
            "  forged: 0.7482\n",
            "  warrantie: 0.7476\n",
            "  hush: 0.7395\n",
            "  obscure: 0.7158\n",
            "  guiltlesse: 0.7157\n",
            "\n",
            "Most similar to 'play':\n",
            "  villanous: 0.6786\n",
            "  hart: 0.6780\n",
            "  considered: 0.6716\n",
            "  vngalled: 0.6537\n",
            "  indifferently: 0.6229\n",
            "\n",
            "'revenge' not found in vocabulary (likely due to frequency or preprocessing).\n",
            "\n",
            "'love' not found in vocabulary (likely due to frequency or preprocessing).\n",
            "\n",
            "'madness' not found in vocabulary (likely due to frequency or preprocessing).\n",
            "\n",
            "'skull' not found in vocabulary (likely due to frequency or preprocessing).\n",
            "\n",
            "Most similar to 'denmark':\n",
            "  beauteous: 0.8753\n",
            "  importing: 0.8082\n",
            "  polake: 0.7999\n",
            "  warres: 0.7989\n",
            "  prison: 0.7419\n",
            "\n",
            "Most similar to 'polonius':\n",
            "  tugging: 0.8731\n",
            "  voltemand: 0.8464\n",
            "  greenly: 0.8422\n",
            "  cornelius: 0.8397\n",
            "  louingly: 0.8169\n",
            "\n",
            "Most similar to 'laertes':\n",
            "  aboord: 0.6293\n",
            "  coffin: 0.6254\n",
            "  foile: 0.6216\n",
            "  faction: 0.6114\n",
            "  riotous: 0.5954\n",
            "\n",
            "Most similar to 'horatio':\n",
            "  wounded: 0.6311\n",
            "  grand: 0.6200\n",
            "  offend: 0.6200\n",
            "  marcellus: 0.6123\n",
            "  holla: 0.6104\n",
            "\n",
            "Most similar to 'gertrude':\n",
            "  claudius: 0.7623\n",
            "  incenst: 0.7593\n",
            "  captaine: 0.6651\n",
            "  greet: 0.6451\n",
            "  transformation: 0.6446\n",
            "\n",
            "Most similar to 'claudius':\n",
            "  louingly: 0.9312\n",
            "  embracing: 0.9223\n",
            "  cum: 0.8858\n",
            "  rosincran: 0.8841\n",
            "  alijs: 0.8816\n",
            "\n",
            "Most similar to 'fortinbras':\n",
            "  compact: 0.8467\n",
            "  slay: 0.8442\n",
            "  inheritance: 0.8096\n",
            "  obeyes: 0.8076\n",
            "  armie: 0.7993\n",
            "\n",
            "'poison' not found in vocabulary (likely due to frequency or preprocessing).\n",
            "\n",
            "Most similar to 'sword':\n",
            "  trophee: 0.7846\n",
            "  hatchment: 0.7813\n",
            "  greekes: 0.7536\n",
            "  striking: 0.7468\n",
            "  mincing: 0.7414\n",
            "\n",
            "Most similar to 'yorick':\n",
            "  knew: 0.8035\n",
            "  storme: 0.8026\n",
            "  manet: 0.7961\n",
            "  sadly: 0.7918\n",
            "  hugger: 0.7812\n",
            "\n",
            "Most similar to 'doubt':\n",
            "  disclose: 0.8055\n",
            "  hatch: 0.7880\n",
            "  ophel: 0.7285\n",
            "  lier: 0.7252\n",
            "  strawes: 0.7181\n",
            "\n",
            "Most similar to 'fate':\n",
            "  willes: 0.9100\n",
            "  contrary: 0.8954\n",
            "  priuy: 0.8700\n",
            "  mutine: 0.8190\n",
            "  matron: 0.8158\n",
            "\n",
            "Most similar to 'honour':\n",
            "  breach: 0.8482\n",
            "  losse: 0.8421\n",
            "  weight: 0.8406\n",
            "  sustaine: 0.8325\n",
            "  exception: 0.8032\n",
            "\n",
            "Pair similarities:\n",
            "  hamlet - ophelia: 0.2943\n",
            "  king - queen: 0.2157\n",
            "  death - ghost: 0.2938\n",
            "  Cannot compare 'play' and 'revenge': one or both not in vocabulary.\n",
            "  Cannot compare 'hamlet' and 'madness': one or both not in vocabulary.\n",
            "  king - claudius: 0.4284\n",
            "  Cannot compare 'ophelia' and 'love': one or both not in vocabulary.\n",
            "  Cannot compare 'laertes' and 'revenge': one or both not in vocabulary.\n",
            "  Cannot compare 'poison' and 'death': one or both not in vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados de similitud de \"Hamlet\"**\n",
        "\n",
        "La evaluación de los embeddings de \"Hamlet\" mediante `most_similar` y la similitud coseno entre pares específicos revela la capacidad y las limitaciones del modelo Word2Vec entrenado con un corpus literario único.\n",
        "\n",
        "**1. Cobertura del vocabulario y su impacto semántico:**\n",
        "Se constata una limitación crítica debida a la ausencia de términos temáticamente centrales como `love`, `madness`, `revenge`, `skull` y `poison` en el vocabulario del modelo. Esta exclusión, atribuible a la frecuencia de los términos post-preprocesamiento o a los umbrales de `min_count`, impide una representación semántica completa de dimensiones narrativas fundamentales de \"Hamlet\". Los embeddings resultantes, por tanto, operan sobre un subconjunto léxico que no abarca la totalidad conceptual de la obra.\n",
        "\n",
        "**2. Captura de asociaciones contextuales específicas (`most_similar`):**\n",
        "El modelo ha demostrado ser efectivo en la captura de asociaciones semánticas altamente específicas y contextualmente relevantes para \"Hamlet\" cuando los términos están presentes en su vocabulario. Esto se fundamenta en la hipótesis distribucional, donde la proximidad vectorial refleja la similitud de los contextos de co-ocurrencia.\n",
        "* Para `ophelia`, se observan asociaciones como `beautifed`, `idoll`, `orizons` y `nimph` (similitudes >0.73), términos directamente vinculados a su caracterización y a las descripciones textuales.\n",
        "* La relación de `queen` con `willow` y `aslant` (similitudes >0.77) es particularmente notable, pues codifica elementos descriptivos específicos de la narrativa de la muerte de `Ophelia` narrada por `Gertrude`.\n",
        "* Para `ghost`, la similitud con `adulterate` (0.6547) refleja una conexión temática central.\n",
        "* Términos como `denmark` y `fortinbras` se asocian con un léxico geopolítico coherente (`polake`, `warres`, `prison` para `denmark`; `compact`, `slay`, `inheritance` para `fortinbras`), con similitudes a menudo >0.79.\n",
        "* El modelo también aprende representaciones para grafías arcaicas (ej., `discouery` para `king`) y formas léxicas específicas del preprocesamiento.\n",
        "* No obstante, se identifican asociaciones menos intuitivas (ej., para `hamlet`: `vnbrac`, `doublet`, `dar`, con similitudes de 0.53-0.58), que podrían interpretarse como ruido estadístico, la influencia de co-ocurrencias esporádicas en un corpus reducido, o la captura de contextos muy locales y específicos (vestimenta, apariencia) en lugar de relaciones semánticas más abstractas. Las puntuaciones de similitud en estos casos tienden a ser moderadas.\n",
        "\n",
        "**3. Relaciones semánticas cuantitativas  (similitud entre pares):**\n",
        "La similitud coseno entre pares seleccionados ofrece una cuantificación de su cercanía en el espacio vectorial aprendido:\n",
        "* La relación `king` - `claudius` (0.4284) indica una asociación semántica moderada, reflejando su identidad parcialmente superpuesta dentro de la narrativa.\n",
        "* Pares como `hamlet` - `ophelia` (0.2943) y `king` - `queen` (0.2157) exhiben similitudes más débiles. Esto puede sugerir que, si bien narrativamente conectados, sus perfiles distribucionales (los contextos lingüísticos en los que aparecen) no son lo suficientemente intercambiables en el corpus como para generar una alta similitud coseno. Alternativamente, la complejidad de sus relaciones podría no traducirse en una simple proximidad vectorial basada en co-ocurrencia.\n",
        "* La similitud `death` - `ghost` (0.2938) es también moderada-baja, posiblemente indicando que, aunque conceptualmente ligados, sus contextos de uso en \"Hamlet\" son suficientemente distintos.\n",
        "* La imposibilidad de calcular similitudes para pares que incluyen términos ausentes del vocabulario (`play`/`revenge`, `hamlet`/`madness`, etc.) reitera el impacto de la cobertura léxica en la capacidad analítica del modelo.\n"
      ],
      "metadata": {
        "id": "6DYLRZViKWPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Test Analogy Tests\n",
        "# ==============================================================================\n",
        "\n",
        "if wv_hamlet:\n",
        "    a1, b1, c1 = \"king\", \"man\", \"woman\"\n",
        "    expected1 = \"queen\"\n",
        "    print(f\"\\nAnalogy Test 1: {a1} - {b1} + {c1} (expecting ~{expected1})\")\n",
        "    if all(word in wv_hamlet for word in [a1, b1, c1]):\n",
        "        try:\n",
        "            results_direct = wv_hamlet.most_similar(positive=[a1, c1], negative=[b1], topn=10)\n",
        "            print(f\"  Directly via most_similar(positive=['{a1}', '{c1}'], negative=['{b1}']):\")\n",
        "            found_direct = any(res[0] == expected1 for res in results_direct)\n",
        "            for word, score in results_direct: print(f\"    {word}: {score:.4f}\")\n",
        "            if found_direct: print(f\"    -> Expected '{expected1}' was found!\")\n",
        "\n",
        "            vec_a1 = wv_hamlet[a1]; vec_b1 = wv_hamlet[b1]; vec_c1 = wv_hamlet[c1]\n",
        "            fabricated_vec1 = vec_a1 - vec_b1 + vec_c1\n",
        "            results_manual = wv_hamlet.most_similar(positive=[fabricated_vec1], topn=10)\n",
        "            print(\"\\n  Manually (vector(king) - vector(man) + vector(woman)) then finding similar:\")\n",
        "            found_manual = any(res[0] == expected1 for res in results_manual)\n",
        "            for word, score in results_manual:\n",
        "                 if word not in [a1,b1,c1]: print(f\"    {word}: {score:.4f}\")\n",
        "            if found_manual: print(f\"    -> Expected '{expected1}' was found!\")\n",
        "            if not found_direct and not found_manual: print(f\"    -> Expected '{expected1}' not prominent.\")\n",
        "        except KeyError as e:\n",
        "            print(f\"  Skipping Analogy Test 1 due to missing word: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  An error occurred during Analogy Test 1: {e}\")\n",
        "    else:\n",
        "        print(f\"  Skipping Analogy Test 1: One or more input words ({a1},{b1},{c1}) not in vocabulary.\")\n",
        "\n",
        "    a2, b2, c2 = \"laertes\", \"polonius\", \"ghost\"\n",
        "    expected2 = \"hamlet\"\n",
        "    print(f\"\\nAnalogy Test 2: {a2} (son1) - {b2} (father1) + {c2} (father2) (expecting ~{expected2} (son2))\")\n",
        "    if all(word in wv_hamlet for word in [a2, b2, c2]):\n",
        "        try:\n",
        "            results_direct2 = wv_hamlet.most_similar(positive=[a2, c2], negative=[b2], topn=5)\n",
        "            print(f\"  Directly via most_similar(positive=['{a2}', '{c2}'], negative=['{b2}']):\")\n",
        "            found_direct2 = any(res[0] == expected2 for res in results_direct2)\n",
        "            for word, score in results_direct2: print(f\"    {word}: {score:.4f}\")\n",
        "            if found_direct2: print(f\"    -> Expected '{expected2}' was found!\")\n",
        "            else: print(f\"    -> Expected '{expected2}' not prominent.\")\n",
        "        except KeyError as e:\n",
        "            print(f\"  Skipping Analogy Test 2 due to missing word: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  An error occurred during Analogy Test 2: {e}\")\n",
        "    else:\n",
        "        print(f\"  Skipping Analogy Test 2: One or more input words ({a2},{b2},{c2}) not in vocabulary.\")\n",
        "else:\n",
        "    print(\"Word2Vec model (wv_hamlet) not available for analogy testing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4eb4_FBDUYT",
        "outputId": "0f1c9d8e-34bf-480d-964a-4e2b1ef93dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analogy Test 1: king - man + woman (expecting ~queen)\n",
            "  Directly via most_similar(positive=['king', 'woman'], negative=['man']):\n",
            "    hide: 0.4740\n",
            "    ignorant: 0.4675\n",
            "    fox: 0.4555\n",
            "    pernicious: 0.4426\n",
            "    smiling: 0.4266\n",
            "    confound: 0.4266\n",
            "    palpable: 0.4243\n",
            "    arraigne: 0.4156\n",
            "    trade: 0.4147\n",
            "    amaze: 0.4099\n",
            "\n",
            "  Manually (vector(king) - vector(man) + vector(woman)) then finding similar:\n",
            "    hide: 0.3585\n",
            "    ignorant: 0.3571\n",
            "    fox: 0.3380\n",
            "    arraigne: 0.3333\n",
            "    palpable: 0.3268\n",
            "    kinde: 0.3244\n",
            "    tribute: 0.3150\n",
            "    bring: 0.3105\n",
            "    -> Expected 'queen' not prominent.\n",
            "\n",
            "Analogy Test 2: laertes (son1) - polonius (father1) + ghost (father2) (expecting ~hamlet (son2))\n",
            "  Directly via most_similar(positive=['laertes', 'ghost'], negative=['polonius']):\n",
            "    lesse: 0.5069\n",
            "    occurrent: 0.4814\n",
            "    anticipation: 0.4404\n",
            "    pittied: 0.4388\n",
            "    exclaim: 0.4105\n",
            "    -> Expected 'hamlet' not prominent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tests de Analogías**\n",
        "\n",
        "Los tests de analogías (king - man + woman ≈ queen; laertes - polonius + ghost ≈ hamlet) no identificaron los términos canónicos. Los vectores resultantes de las operaciones aritméticas presentaron bajas similitudes coseno (0.3-0.5) con los términos más cercanos del vocabulario, como hide o lesse.\n",
        "\n",
        "Este resultado se atribuye principalmente a:\n",
        "\n",
        "Especificidad del Corpus y Ausencia de Subestructuras Lineales Generalizables: Los embeddings, entrenados exclusivamente en \"Hamlet\" (3700 oraciones), reflejan un contexto semántico altamente especializado. Este corpus limitado y de dominio único impide la formación robusta de las subestructuras lineales (vector offsets consistentes) necesarias para que el modelo capture las relaciones semánticas abstractas (género, realeza, roles familiares) que estas analogías evalúan. Las asociaciones narrativas específicas de la obra dominan sobre estas relaciones lingüísticas generales.\n",
        "\n",
        "Pobre Alineación de Vectores de Analogía: Las bajas puntuaciones de similitud indican que los vectores \"fabricados\" por las operaciones de analogía no se alinean fuertemente con ningún vector de palabra existente en el espacio aprendido de \"Hamlet\". Esto sugiere que las relaciones semánticas específicas que se intentan aislar mediante la aritmética vectorial no están representadas como vectores distintivos y bien definidos en este modelo.\n",
        "\n",
        "En conclusión, si bien los embeddings pueden capturar similitudes contextuales locales dentro de \"Hamlet\", el espacio semántico aprendido carece de la generalización y estructura relacional necesarias para resolver estas tareas de analogía, una limitación esperable dada la naturaleza del corpus de entrenamiento."
      ],
      "metadata": {
        "id": "G3EjeWYtX-Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Graph the Resulting Embeddings (t-SNE)\n",
        "# ==============================================================================\n",
        "\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "def reduce_dimensions_class_style(keyed_vectors_obj, num_target_dimensions=2, random_seed=0):\n",
        "\n",
        "    if not keyed_vectors_obj or not hasattr(keyed_vectors_obj, 'vectors') or \\\n",
        "       not hasattr(keyed_vectors_obj, 'index_to_key') or len(keyed_vectors_obj.index_to_key) == 0:\n",
        "        print(\"Error: KeyedVectors object is None, empty, or not a valid Gensim KeyedVectors instance.\")\n",
        "        return None, None\n",
        "\n",
        "    all_vectors = np.asarray(keyed_vectors_obj.vectors)\n",
        "    all_labels = np.asarray(keyed_vectors_obj.index_to_key)\n",
        "\n",
        "    default_perplexity = 30.0\n",
        "    if len(all_vectors) <= default_perplexity or len(all_vectors) <= num_target_dimensions:\n",
        "        print(\n",
        "            f\"Warning: Not enough samples ({len(all_vectors)}) for t-SNE with default perplexity ({default_perplexity}) \"\n",
        "            f\"and/or target dimensions ({num_target_dimensions}). t-SNE might fail or produce poor results. \"\n",
        "            \"Consider a smaller vocabulary or different t-SNE parameters if issues arise.\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        tsne_reducer = TSNE(n_components=num_target_dimensions, random_state=random_seed)\n",
        "        transformed_vectors = tsne_reducer.fit_transform(all_vectors)\n",
        "        return transformed_vectors, all_labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error during t-SNE transformation (n_components={num_target_dimensions}): {e}\")\n",
        "        return None, all_labels\n",
        "\n",
        "# --- Perform Dimensionality Reductions ---\n",
        "transformed_vecs_2d = None\n",
        "all_vocab_labels_2d = None\n",
        "transformed_vecs_3d = None\n",
        "all_vocab_labels_3d = None\n",
        "\n",
        "\n",
        "if wv_hamlet:\n",
        "    print(\"Reducing dimensions for 2D t-SNE plot...\")\n",
        "    transformed_vecs_2d, all_vocab_labels_2d = reduce_dimensions_class_style(wv_hamlet, 2, RANDOM_STATE)\n",
        "    if transformed_vecs_2d is not None:\n",
        "        print(f\"2D projection generated for {len(all_vocab_labels_2d)} words.\")\n",
        "    else:\n",
        "        print(\"Failed to generate 2D projection.\")\n",
        "\n",
        "    print(\"\\nReducing dimensions for 3D t-SNE plot...\")\n",
        "\n",
        "    transformed_vecs_3d, all_vocab_labels_3d = reduce_dimensions_class_style(wv_hamlet, 3, RANDOM_STATE)\n",
        "    if transformed_vecs_3d is not None:\n",
        "        print(f\"3D projection generated for {len(all_vocab_labels_3d)} words.\")\n",
        "    else:\n",
        "        print(\"Failed to generate 3D projection.\")\n",
        "else:\n",
        "    print(\"Word2Vec model's KeyedVectors (wv_hamlet) not available. Skipping t-SNE.\")\n",
        "\n",
        "MAX_WORDS_TO_DISPLAY = 200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKrc0WQsxjbI",
        "outputId": "10ffe15d-0783-4541-f3f9-f95dd2e33eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reducing dimensions for 2D t-SNE plot...\n",
            "2D projection generated for 4145 words.\n",
            "\n",
            "Reducing dimensions for 3D t-SNE plot...\n",
            "3D projection generated for 4145 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las visualizaciones t-SNE (t-Distributed Stochastic Neighbor Embedding) constituyen una técnica no lineal de reducción de dimensionalidad usada para representar relaciones de similitud semántica entre palabras, a partir de sus embeddings vectoriales. Basadas en la hipótesis distribucional, según la cual las palabras que aparecen en contextos similares tienden a compartir significados, estas visualizaciones buscan preservar las relaciones de vecindad local del espacio original de alta dimensión. Los embeddings generados por modelos como Word2Vec capturan patrones de coocurrencia del corpus (en este caso, Hamlet) y los proyectan en un espacio vectorial. El algoritmo t-SNE, al reducir estas representaciones a 2D o 3D, facilita una inspección cualitativa de la semántica aprendida, donde la proximidad espacial entre puntos es un proxy de similitud contextual."
      ],
      "metadata": {
        "id": "vzVkasjM0CZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "if transformed_vecs_2d is not None and all_vocab_labels_2d is not None:\n",
        "    num_display = min(MAX_WORDS_TO_DISPLAY, len(all_vocab_labels_2d))\n",
        "\n",
        "    print(f\"\\n--- Plotting 2D t-SNE Projection for top {num_display} words ---\")\n",
        "    try:\n",
        "        fig_2d = px.scatter(\n",
        "            x=transformed_vecs_2d[:num_display, 0],\n",
        "            y=transformed_vecs_2d[:num_display, 1],\n",
        "            text=all_vocab_labels_2d[:num_display],\n",
        "            title=f\"2D t-SNE Projection (Hamlet Embeddings, Top {num_display} Words)\"\n",
        "        )\n",
        "        fig_2d.update_traces(textposition='top center', mode='markers+text', marker={'size': 5})\n",
        "        fig_2d.update_layout(height=800, width=1000, xaxis_title=\"t-SNE Dimension 1\", yaxis_title=\"t-SNE Dimension 2\")\n",
        "        fig_2d.show(renderer=\"colab\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying 2D Plotly figure: {e}\")\n",
        "else:\n",
        "    print(\"Data for 2D plot (transformed_vecs_2d or labels) not available. Check Cell 1.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "6xskKGJFx4p3",
        "outputId": "a6c87999-ab42-424c-f4ce-19f64c1fda6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Plotting 2D t-SNE Projection for top 200 words ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"9c3cae4a-30ba-463c-9333-056fa2d6c92d\" class=\"plotly-graph-div\" style=\"height:800px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9c3cae4a-30ba-463c-9333-056fa2d6c92d\")) {                    Plotly.newPlot(                        \"9c3cae4a-30ba-463c-9333-056fa2d6c92d\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"size\":5},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"ham\",\"lord\",\"king\",\"haue\",\"come\",\"let\",\"shall\",\"hamlet\",\"thou\",\"good\",\"hor\",\"thy\",\"wa\",\"oh\",\"like\",\"enter\",\"make\",\"father\",\"well\",\"doe\",\"would\",\"know\",\"selfe\",\"may\",\"sir\",\"loue\",\"qu\",\"giue\",\"thee\",\"laer\",\"go\",\"must\",\"ile\",\"hath\",\"ophe\",\"speake\",\"man\",\"time\",\"vpon\",\"heere\",\"pol\",\"one\",\"say\",\"see\",\"mother\",\"mine\",\"much\",\"rosin\",\"heauen\",\"tell\",\"night\",\"thinke\",\"thus\",\"horatio\",\"queene\",\"polon\",\"yet\",\"take\",\"play\",\"death\",\"laertes\",\"eye\",\"vp\",\"againe\",\"th\",\"soule\",\"thing\",\"mar\",\"heart\",\"god\",\"owne\",\"friend\",\"heare\",\"looke\",\"could\",\"life\",\"dead\",\"might\",\"hold\",\"word\",\"matter\",\"made\",\"pray\",\"thought\",\"ophelia\",\"nothing\",\"hand\",\"away\",\"clo\",\"whose\",\"hast\",\"indeed\",\"ha\",\"downe\",\"nay\",\"doth\",\"head\",\"nature\",\"leaue\",\"put\",\"day\",\"world\",\"neuer\",\"call\",\"set\",\"ghost\",\"sweet\",\"though\",\"player\",\"follow\",\"euen\",\"done\",\"two\",\"madnesse\",\"gentleman\",\"guild\",\"faire\",\"denmarke\",\"seene\",\"body\",\"earth\",\"wee\",\"shew\",\"poore\",\"hee\",\"himselfe\",\"mad\",\"within\",\"true\",\"deere\",\"feare\",\"england\",\"end\",\"sonne\",\"great\",\"polonius\",\"till\",\"meanes\",\"spirit\",\"osr\",\"stand\",\"part\",\"still\",\"long\",\"daughter\",\"beleeue\",\"sword\",\"keepe\",\"eare\",\"speech\",\"old\",\"goe\",\"welcome\",\"better\",\"art\",\"rest\",\"tongue\",\"guildensterne\",\"first\",\"brother\",\"noble\",\"bee\",\"farewell\",\"since\",\"ere\",\"many\",\"betweene\",\"fortune\",\"marry\",\"exit\",\"last\",\"breath\",\"purpose\",\"grace\",\"fit\",\"youth\",\"therefore\",\"drinke\",\"liue\",\"kin\",\"cause\",\"heard\",\"lady\",\"fortinbras\",\"thine\",\"reason\",\"vse\",\"little\",\"else\",\"gone\",\"vertue\",\"without\",\"beare\",\"gertrude\",\"reynol\",\"said\",\"saw\",\"marke\",\"question\",\"sent\"],\"x\":[29.283138,41.61216,31.642277,23.22685,52.868156,55.570194,11.682569,43.44889,63.215168,67.78691,43.191414,1.3870493,6.703864,9.4148035,-17.485312,37.84162,20.865625,0.10339869,67.554146,31.565063,-0.49369475,-5.5657086,-18.9769,-24.99686,39.57982,-14.086336,18.748055,-1.1157613,54.340282,57.935863,65.56378,0.16804107,22.02637,-71.49688,41.11955,23.633427,-13.5910635,-78.095345,52.795696,-1.1894704,60.90869,-24.394566,44.883358,37.2442,10.777956,18.051285,16.417341,48.711166,-12.284301,23.69877,4.7411184,59.989708,-40.53238,57.047546,31.793758,74.327705,23.370438,-9.328036,33.23574,0.61036795,35.342735,-36.711185,-45.341415,19.157022,-29.404058,-27.60671,-29.911135,53.960506,-52.846268,61.613625,-27.839249,68.909164,57.329823,18.474125,33.849472,-26.107944,-8.503484,26.801762,-37.803185,-36.733746,36.70975,-27.54164,68.48911,-19.307991,11.148095,27.593111,-23.21613,51.713924,6.475809,-29.80167,61.52884,33.36362,60.799328,-27.569838,53.340244,-52.07513,-42.121143,-33.335453,71.72378,-19.698578,7.5452046,5.3966646,-4.118148,31.04227,-25.016788,35.141026,3.4851274,-32.5337,10.083611,54.284218,-25.138586,28.560875,29.942955,-7.1507907,58.24638,51.918518,7.579219,-16.592758,-6.9272523,-10.763781,-6.954681,9.084934,22.744614,16.893425,36.58635,-78.1159,7.045584,20.272049,-27.659431,25.078691,-3.6344452,11.235569,25.78975,36.2934,-18.33538,35.26795,-11.986588,72.99266,-35.273693,48.224583,25.636637,43.89415,63.61313,69.93343,65.57704,21.226799,-45.48531,16.05781,-18.140265,42.79005,-35.336205,56.518383,63.52781,37.308865,57.898895,58.18538,-27.425657,36.421326,1.8252788,31.380465,-3.7010832,19.01589,64.91051,-43.3143,-59.43058,-38.310192,-57.20812,18.647621,74.815,37.43412,0.0627048,-15.344065,-9.900244,-64.44566,-21.871904,-39.621017,14.527602,1.9613682,40.00953,46.143158,46.318745,39.01775,10.573397,45.100456,44.189774,-34.60032,-4.715814,-23.993885,4.948588,-9.923995,-42.1017,28.252068,-1.8389115,40.006847,63.082832,70.07345,52.486233,58.558575,46.77001,40.085743],\"xaxis\":\"x\",\"y\":[3.6921647,-14.193916,24.256618,-8.900661,-0.52166486,-8.55996,20.48012,34.1225,-13.50249,13.408654,-7.4895244,20.049726,-19.841547,35.60116,16.861929,52.918053,-21.835455,9.982113,-1.6285269,-2.365759,-30.630705,-9.353225,-54.56134,-47.85474,7.685166,-17.334429,32.049763,22.051546,-41.58037,-15.917322,31.451738,-59.33777,4.8218174,-20.224758,-15.610689,29.362635,35.342068,-16.301397,35.82315,32.942204,5.0053363,18.253246,5.6727905,-31.902472,44.960007,60.267742,-68.023026,-9.541578,-54.924397,-41.496845,-52.775814,-10.083641,55.56195,28.775318,55.57301,2.7899828,-61.44575,-72.22023,-47.20639,2.230495,58.374535,-23.57374,35.36835,11.044971,-3.3825498,26.842152,-40.763523,23.065935,-27.200245,-1.4277688,14.954179,17.798655,-9.870306,25.043482,-37.207466,34.65577,54.451687,-33.632454,30.821522,-32.82204,-57.841335,-63.852757,-33.227726,-23.766973,67.31058,-52.774128,-26.256878,-27.676416,-26.051245,6.5964894,-17.261324,-11.109987,22.073065,65.77035,-10.032668,-41.17123,48.56254,-51.393185,11.5509,-65.220955,-47.61703,-73.76262,46.557648,29.528456,67.10966,46.344345,56.270077,-14.490491,22.803293,-14.668074,-1.5956112,-42.76056,43.02256,-19.01431,7.710071,13.777346,62.791916,46.96231,-42.585693,55.934895,-51.955322,-29.941128,-46.766125,70.198296,0.78977466,-2.185,-24.560736,44.96785,-57.474125,48.060658,39.477955,3.8331707,-17.760525,36.376453,-31.648586,52.550274,-7.985199,-27.727959,20.693193,9.571369,-22.369385,-28.877825,-27.710133,-17.878403,-25.587828,-63.815674,-8.21979,-15.864036,-4.3556876,-54.596626,39.737926,-22.508818,12.114867,-37.014828,25.268145,-32.534996,-24.342026,55.66217,-11.553705,40.34662,13.570437,-32.73274,-9.569366,-51.765324,30.216454,26.777481,-20.761583,37.856083,-5.8778596,52.303505,-43.370228,-31.861,-32.427837,-29.910818,-15.844941,-52.873856,-31.223341,27.678852,-6.289179,30.739933,2.4528883,-47.71967,51.90474,48.57899,26.206213,9.119677,25.8372,11.612043,-61.714672,50.392586,-46.383965,-15.60803,-33.319393,55.340965,3.9538577,4.338017,-48.509735,46.502445,-10.642941,23.789818],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"t-SNE Dimension 1\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"t-SNE Dimension 2\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"2D t-SNE Projection (Hamlet Embeddings, Top 200 Words)\"},\"height\":800,\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9c3cae4a-30ba-463c-9333-056fa2d6c92d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2D t-SNE**\n",
        "\n",
        "En la proyección 2D se observa una organización general útil para captar agrupamientos temáticos, aunque con cierto solapamiento en regiones de alta densidad. Es destacable la agrupación densa en la parte superior derecha, donde coexisten \"hamlet\", \"claudius\", \"ghost\", \"horatio\", \"king\", \"queen\", \"gertrude\" y \"polonius\", lo que refleja la prominencia de estos personajes en contextos comunes. Sin embargo, la bidimensionalidad impone ciertas limitaciones: palabras clave como \"himselfe\", \"ere\", \"grace\" o \"sword\" aparecen más aisladas, sin indicar vínculos explícitos, aunque semánticamente relevantes. La ausencia de profundidad dificulta la observación de conexiones entre capas semánticas intermedias y palabras periféricas de baja frecuencia, algunas de las cuales podrían estar artificialmente alejadas del núcleo por limitaciones inherentes al modelo t-SNE en 2D.\n"
      ],
      "metadata": {
        "id": "A1zISy4Cz_Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "if transformed_vecs_3d is not None and all_vocab_labels_3d is not None:\n",
        "    # Ensure MAX_WORDS_TO_DISPLAY is not greater than available words\n",
        "    num_display = min(MAX_WORDS_TO_DISPLAY, len(all_vocab_labels_3d))\n",
        "\n",
        "    print(f\"\\n--- Plotting 3D t-SNE Projection for top {num_display} words ---\")\n",
        "    try:\n",
        "        fig_3d = px.scatter_3d(\n",
        "            x=transformed_vecs_3d[:num_display, 0],\n",
        "            y=transformed_vecs_3d[:num_display, 1],\n",
        "            z=transformed_vecs_3d[:num_display, 2],\n",
        "            text=all_vocab_labels_3d[:num_display],\n",
        "            title=f\"3D t-SNE Projection (Hamlet Embeddings, Top {num_display} Words)\"\n",
        "        )\n",
        "        fig_3d.update_traces(marker_size=3, mode='markers+text')\n",
        "        fig_3d.update_layout(height=800, width=1000, scene=dict(aspectmode='data'))\n",
        "        fig_3d.show(renderer=\"colab\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying 3D Plotly figure: {e}\")\n",
        "else:\n",
        "    print(\"Data for 3D plot (transformed_vecs_3d or labels) not available. Check Cell 1.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "VA6Oor5tyC0l",
        "outputId": "7fba20e6-e8b3-431e-8c69-aa045434aaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Plotting 3D t-SNE Projection for top 200 words ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d546af1a-1d5f-47f7-b7c5-643a90c09fc1\" class=\"plotly-graph-div\" style=\"height:800px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d546af1a-1d5f-47f7-b7c5-643a90c09fc1\")) {                    Plotly.newPlot(                        \"d546af1a-1d5f-47f7-b7c5-643a90c09fc1\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003ez=%{z}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers+text\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"text\":[\"ham\",\"lord\",\"king\",\"haue\",\"come\",\"let\",\"shall\",\"hamlet\",\"thou\",\"good\",\"hor\",\"thy\",\"wa\",\"oh\",\"like\",\"enter\",\"make\",\"father\",\"well\",\"doe\",\"would\",\"know\",\"selfe\",\"may\",\"sir\",\"loue\",\"qu\",\"giue\",\"thee\",\"laer\",\"go\",\"must\",\"ile\",\"hath\",\"ophe\",\"speake\",\"man\",\"time\",\"vpon\",\"heere\",\"pol\",\"one\",\"say\",\"see\",\"mother\",\"mine\",\"much\",\"rosin\",\"heauen\",\"tell\",\"night\",\"thinke\",\"thus\",\"horatio\",\"queene\",\"polon\",\"yet\",\"take\",\"play\",\"death\",\"laertes\",\"eye\",\"vp\",\"againe\",\"th\",\"soule\",\"thing\",\"mar\",\"heart\",\"god\",\"owne\",\"friend\",\"heare\",\"looke\",\"could\",\"life\",\"dead\",\"might\",\"hold\",\"word\",\"matter\",\"made\",\"pray\",\"thought\",\"ophelia\",\"nothing\",\"hand\",\"away\",\"clo\",\"whose\",\"hast\",\"indeed\",\"ha\",\"downe\",\"nay\",\"doth\",\"head\",\"nature\",\"leaue\",\"put\",\"day\",\"world\",\"neuer\",\"call\",\"set\",\"ghost\",\"sweet\",\"though\",\"player\",\"follow\",\"euen\",\"done\",\"two\",\"madnesse\",\"gentleman\",\"guild\",\"faire\",\"denmarke\",\"seene\",\"body\",\"earth\",\"wee\",\"shew\",\"poore\",\"hee\",\"himselfe\",\"mad\",\"within\",\"true\",\"deere\",\"feare\",\"england\",\"end\",\"sonne\",\"great\",\"polonius\",\"till\",\"meanes\",\"spirit\",\"osr\",\"stand\",\"part\",\"still\",\"long\",\"daughter\",\"beleeue\",\"sword\",\"keepe\",\"eare\",\"speech\",\"old\",\"goe\",\"welcome\",\"better\",\"art\",\"rest\",\"tongue\",\"guildensterne\",\"first\",\"brother\",\"noble\",\"bee\",\"farewell\",\"since\",\"ere\",\"many\",\"betweene\",\"fortune\",\"marry\",\"exit\",\"last\",\"breath\",\"purpose\",\"grace\",\"fit\",\"youth\",\"therefore\",\"drinke\",\"liue\",\"kin\",\"cause\",\"heard\",\"lady\",\"fortinbras\",\"thine\",\"reason\",\"vse\",\"little\",\"else\",\"gone\",\"vertue\",\"without\",\"beare\",\"gertrude\",\"reynol\",\"said\",\"saw\",\"marke\",\"question\",\"sent\"],\"x\":[15.901575,26.16989,15.321291,5.8281484,31.986475,25.122198,1.2173753,24.104755,24.703009,28.90041,25.832691,3.1375864,7.3914223,12.879511,-7.635977,21.768814,14.1181,9.431768,38.040016,10.996053,5.7786336,-10.34026,9.155619,-9.777312,17.671528,0.41066024,14.916472,7.022751,16.007486,17.86399,24.855984,6.1308765,41.08402,-23.303854,24.783192,-1.4158413,-11.450027,-20.92984,-15.887215,0.050163012,37.203495,3.1398754,24.690987,23.566301,4.9446535,-0.85026276,8.213087,23.538013,-3.1549351,13.711869,6.7764215,29.91943,-28.488369,27.365242,12.165237,36.89488,1.2625015,-3.5734036,12.033291,-6.6089826,15.904856,-12.411141,-19.443617,11.25701,-11.225603,-17.767551,-30.098839,15.4877405,-13.010544,30.518478,-9.771782,5.160067,36.00355,5.6221786,9.238451,-15.459161,-14.249128,6.127458,-17.29719,-6.5624204,25.085932,-7.240149,33.0123,5.262191,7.1020355,21.482946,-15.889973,24.460632,21.750292,-11.381248,27.693579,16.094967,29.452581,-33.727234,37.623123,-23.884132,-35.485313,0.90596515,1.9273537,-23.964363,-1.6445106,6.0928917,-4.9515414,8.565425,-30.806732,26.03229,3.7928195,-2.0632012,6.202849,27.538504,-24.594446,12.250872,10.5508,3.0290565,28.76203,27.410608,5.147751,8.751204,-0.5885973,-11.252488,1.1231596,-4.182454,21.930662,13.372012,14.007596,-8.294574,19.17278,7.9444566,6.161883,19.935865,-11.034792,31.793676,25.903978,24.21699,-21.234348,18.730349,-4.7487087,27.368193,-37.966225,33.12919,18.628357,26.928356,37.202938,31.597662,41.804485,5.540285,-30.866344,10.804141,-11.640296,26.224396,-35.306076,29.353012,33.831802,21.423687,21.673977,36.420666,-19.141125,17.492073,3.5559306,11.59,8.729751,10.296101,18.72077,-19.262115,-39.8737,-26.118196,-26.774237,-5.6197066,34.6519,21.18507,-3.6887023,-10.093443,-3.1548831,-16.086702,-9.351013,-10.779128,18.599894,-4.9181557,22.143866,27.589975,-8.729632,7.330663,6.823747,28.05554,22.495552,-28.79432,-9.367558,-10.002441,6.9627113,-13.444733,-11.43168,18.29479,7.1817017,12.5411,33.80083,41.579865,24.451468,22.546467,22.529524,26.584156],\"y\":[-6.9887033,-1.7055844,16.197369,11.332925,3.0345745,5.922388,5.9487114,22.86221,12.855098,10.639671,-7.4529138,12.913172,-9.7851925,33.778896,-19.015585,29.479818,-10.752521,21.422306,-3.693039,-36.615475,-14.422182,21.175661,-25.277569,-22.955753,-4.1478815,-11.317868,24.315922,-5.7851534,-22.616453,0.118154846,28.484797,-29.683168,5.665225,1.9960531,0.08890016,-18.506044,32.249657,-0.33750835,35.095047,21.083557,3.608835,-9.918373,1.9333997,-8.399472,13.635093,19.564123,-42.518307,-15.033195,-21.38952,-33.94064,15.711587,-4.1306233,24.47456,24.35226,30.43241,-11.114379,-26.952309,-11.50098,0.8216374,9.745549,25.965584,-33.432247,18.332731,12.221174,-15.833402,7.944739,-25.116257,19.368914,0.4701804,0.72783685,16.092442,41.165764,4.9687815,32.1846,-21.192167,10.825095,26.964808,-19.423296,4.0581303,-21.216759,-23.707497,-0.46749318,-7.4470396,-34.800568,38.830025,-24.169245,-31.155338,-5.178084,-9.66776,-13.599962,6.7381372,-8.224461,17.491135,21.946962,4.4295664,-12.991876,16.581373,-39.316696,14.551386,9.10268,18.699697,-20.195267,8.771716,19.723442,27.538492,30.360178,29.15876,-25.215706,3.6002786,-1.3877301,-31.859629,5.31259,25.26256,-29.719404,6.0320807,6.0668473,35.96255,46.633286,-0.21913843,28.075502,-21.379875,-17.010695,-33.073128,38.81924,-31.249113,-40.345284,-14.762015,14.378555,-26.122417,22.269697,7.449763,14.287981,-16.484362,29.646784,-8.045711,28.444912,22.476418,-13.092684,15.182558,0.5151251,-17.332943,-9.0936575,2.291509,12.394936,-2.5043783,-28.215271,-0.0022361903,-11.783491,19.83547,-28.368872,12.756379,-19.17961,8.532509,-3.6073244,19.568237,-8.885205,-31.100456,31.849854,12.501225,31.336178,16.513815,-29.962112,14.715659,-20.90166,-5.5449033,0.3357907,-24.79345,36.22129,-13.821803,27.436592,14.325315,6.180998,-5.1304264,-21.37616,5.1362376,-15.109358,-1.9429343,23.715475,-1.5318074,20.134455,-12.412107,-1.038127,20.782827,19.494,17.636715,-20.50308,38.44326,-28.381075,-34.536037,24.679838,-11.671212,-13.867312,-19.475689,31.83742,-0.54056907,-1.6890278,-24.508371,37.33592,-20.899128,13.480724],\"z\":[5.453054,26.303465,11.16805,9.742507,-4.861752,-0.0030793916,39.351837,9.683239,-24.170885,20.660114,12.769497,-26.863482,7.882388,-17.997229,26.380585,15.72426,-5.806842,-14.910368,13.629908,11.661182,-25.236225,4.334018,-13.401562,-11.438029,13.997808,21.21809,-23.71819,-29.704233,-22.611029,-7.782705,-24.148481,-19.969099,-3.054468,33.814213,28.870893,29.879093,-9.765614,23.8253,1.2909604,-21.32742,15.701838,26.870459,9.674796,-14.724475,23.510454,10.457396,-3.9958978,20.421093,-17.616331,-2.8648996,-5.9447665,-3.7297485,3.2911837,-12.724786,18.428722,16.621035,8.104805,-40.38924,-20.361223,-13.809957,20.809431,-2.6391966,-24.922436,-5.155705,6.1203275,-14.212165,-4.6674504,-1.6944002,-24.529024,11.4206505,18.114607,-11.109023,-0.09206469,-8.434664,4.04896,5.271211,11.993753,-9.456953,-28.536674,0.10231151,-6.692856,38.23606,-18.188395,22.661898,6.6068816,9.265415,-15.950376,-27.410816,9.105034,-1.2128438,-23.22422,32.641033,-15.617071,-9.15558,4.935808,-22.880714,1.5303055,7.996008,-39.76642,-7.501356,-2.0980349,-36.841755,33.207138,8.708193,-9.512582,11.967394,27.943558,6.386163,38.108677,2.5846004,-15.949709,-22.941914,7.329238,23.874731,4.9175863,23.04429,13.457804,-5.3823547,-21.56863,15.50635,-16.250559,11.330227,-0.26535678,1.0814865,18.448408,11.657863,6.418867,33.956566,30.958588,-2.2981827,-33.636444,0.35743913,-0.21387096,-3.7440972,22.382824,18.796583,-15.817039,8.821169,-19.99242,22.311207,-3.8054845,-3.5784273,-8.610256,-13.720992,-7.0012774,11.15305,-15.835335,17.821615,-19.089668,-12.7471,11.719397,13.562791,18.708353,-38.5581,-15.807572,-3.9948134,5.8278246,14.844515,-17.211897,5.570722,-23.705198,-16.983957,-28.623896,-31.020994,-13.239092,9.130294,16.639551,-7.465313,7.718265,14.885606,-14.636736,-18.065372,-29.904116,-11.434998,15.2646675,-15.985964,-32.984882,-23.55235,-3.4143217,1.6634735,41.36086,-12.616443,23.858318,16.774464,-3.0667012,7.2228947,-15.700551,25.590912,-15.439111,30.853127,-22.27179,23.596693,-22.333595,13.791029,16.755827,4.3288636,-22.078505,-9.454289,20.95555,6.5142226],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}},\"aspectmode\":\"data\"},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"3D t-SNE Projection (Hamlet Embeddings, Top 200 Words)\"},\"height\":800,\"width\":1000},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d546af1a-1d5f-47f7-b7c5-643a90c09fc1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3D t-SNE**\n",
        "\n",
        "Las visualizaciones tridimensionales ofrecen una representación más rica y matizada del espacio semántico. En el gráfico 3D, los clusters observados en la vista 2D se reafirman pero se organizan de manera más coherente. Por ejemplo, el grupo \"hamlet\", \"claudius\", \"ghost\", \"horatio\", \"king\", \"queen\", \"gertrude\" aparece en una región más cohesiva, con una clara estructura esférica que indica fuerte cohesión semántica local. Además, se revelan subestructuras que no son evidentes en 2D: términos como \"madness\", \"death\", \"revenge\" y \"father\" parecen formar un subclúster temático que representa la trama psicológica de la obra. Palabras aisladas en la vista 2D (como \"himselfe\", \"grace\", \"take\", \"earth\") se integran de manera más natural en el espacio tridimensional, sugiriendo que su exclusión aparente era más un artefacto de la reducción bidimensional. En conjunto, las vistas 3D no solo confirman agrupamientos semánticos esperados, sino que revelan gradientes y transiciones contextuales que aportan una capa adicional de interpretabilidad.\n"
      ],
      "metadata": {
        "id": "M8r4qORZgee9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxvwGiF_lCkE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}