## ADRI√ÅN LAPAZ (1706)
# Desaf√≠o 1: calsificaci√≥n de texto

Este desafio aborda un desaf√≠o de Procesamiento del Lenguaje Natural (PLN) utilizando el conjunto de datos **20 Newsgroups** desde *scikit-learn*. El objetivo es explorar t√©cnicas de vectorizaci√≥n, clasificaci√≥n de texto y an√°lisis de similaridad sem√°ntica entre palabras.

## Descripci√≥n del desaf√≠o

El desaf√≠o se divide en tres partes principales:
1.  **Similaridad de documentos**: vectorizar el *corpus* de texto y analizar la similaridad del coseno entre documentos para evaluar la coherencia tem√°tica.
2.  **Clasificaci√≥n de texto**: entrenar y optimizar modelos de clasificaci√≥n Na√Øve Bayes (*MultinomialNB* y *ComplementNB*) para predecir la categor√≠a de un documento, maximizando la m√©trica *f1-score macro*.
3.  **Similaridad de palabras**: transponer la matriz documento-t√©rmino para crear vectores de palabras y analizar las relaciones sem√°nticas entre t√©rminos seleccionados manualmente.


## Resultados principales

### 1. Similaridad entre documentos
Se seleccionaron 5 documentos al azar y se calcularon sus 5 vecinos m√°s similares usando la similaridad del coseno.

- **Coherencia tem√°tica**: se observ√≥ que los documentos con mayor similaridad a menudo pertenecen a la misma categor√≠a, por ejemplo *rec.sport.hockey* y *rec.sport.baseball* tuvieron una similitud de coseno de 0.37.
- **Restulados**: los valores de similaridad de coseno fueron moderados o bajos (entre 0.14 y 0.37). Esto sugiere que, si bien la similaridad del coseno puede agrupar temas, podr√≠a no ser el m√©todo m√°s robusto para una clasificaci√≥n precisa por s√≠ solo.

### 2. Clasificaci√≥n con Na√Øve Bayes
Se utiliz√≥ **optimizaci√≥n bayesiana (*BayesSearchCV*)** para encontrar los mejores hiperpar√°metros tanto para el vectorizador *TfidfVectorizer* como para los clasificadores *MultinomialNB* y *ComplementNB*.

- **Rendimiento en entrenamiento (CV, Cross-Validation)**:
  - **MultinomialNB**: mejor *F1-score* (CV) de **0.7626**.
  - **ComplementNB**: mejor *F1-score* (CV) de **0.7661**.
- **Rendimiento en los datos de testeo**:
  - **MultinomialNB**: *F1-score* en Test de **0.6876**.
  - **ComplementNB**: *F1-score* en Test de **0.6969**.

Ambos modelos mostraron un rendimiento muy similar, aunque *ComplementNB* fue apenas superior. El an√°lisis de hiperpar√°metros revel√≥ que *ComplementNB* logr√≥ su mejor rendimiento con un filtrado din√°mico de vocabulario y un suavizado mayor, sugiriendo una mayor robustez frente al ruido l√©xico.

### 3. Similaridad entre palabras
Se analiz√≥ la similaridad entre 5 palabras seleccionadas (*ball*, *doctor*, *python*, *space*, *water*) tras transponer la matriz *TF-IDF*.

- **Captura de contexto tem√°tico y/o sem√°ntica**: el an√°lisis demostr√≥ la capacidad del modelo para identificar relaciones contextuales muy espec√≠ficas.
  - La asociaci√≥n m√°s fuerte fue entre **python** y **monty** (similaridad de coseno de **0.7138**), una clara referencia sem√°ntica al grupo de comedia *"Monty Python"*.
  - Se encontraron fuertes agrupaciones tem√°ticas, como **doctor** con **receptionist** (0.4392) y **space** con **nasa** (0.3304).
  - El modelo prob√≥ ser altamente dependiente del contexto del corpus: la palabra *water* no se relacion√≥ con la naturaleza, sino con infraestructura urbana (*towers*, *dpw*, *croton*), reflejando los temas de discusi√≥n en los datos.
  
  La t√©cnica de transponer la matriz fue muy efectiva para descubrir conexiones tem√°ticas, sem√°nticas y contextuales entre las palabras, ofreciendo una visi√≥n profunda de c√≥mo se utilizan las palabras dentro del conjunto de datos.

---

## Metodolog√≠a y herramientas

- **Librer√≠as principales**: *scikit-learn*, *numpy*, *skopt*.
- **Vectorizaci√≥n**: *TfidfVectorizer*.
- **Modelos**: *MultinomialNB*, *ComplementNB*.
- **M√©trica de evaluaci√≥n**: *f1_score* (*macro average*).
- **T√©cnica de optimizaci√≥n**: b√∫squeda bayesiana (*BayesSearchCV*) para una sintonizaci√≥n eficiente de hiperpar√°metros.

## Conclusi√≥n
La **similaridad del coseno** es √∫til para la exploraci√≥n tem√°tica, pero los modelos de clasificaci√≥n como **Na√Øve Bayes** son superiores para tareas de predicci√≥n. Adem√°s, el an√°lisis de similaridad de palabras sobre la matriz transpuesta revel√≥ ser una t√©cnica muy poderosa para descubrir **relaciones sem√°nticas** en el texto.

---
---

# Desaf√≠o 2: *Word Embeddings* para *Hamlet* de Shakespeare

[Archivo original en Colab](https://colab.research.google.com/drive/1-3nsIWYq2D5WzH5Ume3_fTlERh8xvQrz?usp=sharing#scrollTo=leSnYEBkCsii)

El objetivo principal de este desaf√≠o es explorar las relaciones sem√°nticas dentro del texto de "Hamlet" aplicando *Word2Vec*, una popular t√©cnica de *Word Embeddings*. El *notebook* gu√≠a a trav√©s de los siguientes pasos:

- **Extracci√≥n y preprocesamiento de texto**: aislamiento del di√°logo de la versi√≥n del corpus *Gutenberg* de "Hamlet", limpieza del texto y aplicaci√≥n de tokenizaci√≥n, lematizaci√≥n y eliminaci√≥n de *stopwords*.
- **Entrenamiento del modelo *Word2Vec***: entrenamiento de un modelo *Word2Vec* *skip-gram* utilizando la librer√≠a *Gensim* para aprender representaciones vectoriales densas (*embeddings*) para palabras bas√°ndose en sus patrones de co-ocurrencia.
- **An√°lisis sem√°ntico**: evaluaci√≥n de los *embeddings* aprendidos mediante consultas de similitud de palabras y pruebas de analog√≠a.
- **Reducci√≥n de dimensionalidad y visualizaci√≥n**: uso de *t-SNE* para reducir los vectores de palabras de alta dimensi√≥n a 2D y 3D para la exploraci√≥n visual de cl√∫steres y relaciones de palabras.

---

## Objetivos del desaf√≠o

Este desaf√≠o fue desarrollado para cumplir con los siguientes √≠tems planteados:

- **Crear los propios vectores con *Gensim* basado en lo visto en clase con otro *dataset***: Se ha utilizado el corpus de "Hamlet" de *NLTK* para entrenar un modelo *Word2Vec* desde cero, generando *embeddings* vectoriales espec√≠ficos para esta obra.
- **Probar t√©rminos de inter√©s y explicar similitudes en el espacio de *embeddings***: Se han seleccionado un conjunto de palabras relevantes de "Hamlet" y se ha analizado su similitud a trav√©s de la funci√≥n *most_similar* de *Gensim*, explicando las asociaciones contextuales y sem√°nticas observadas. Tambi√©n se realizaron pruebas de analog√≠a.
- **Graficarlos**: Se han generado visualizaciones en 2D y 3D de los *Word Embeddings* utilizando *t-SNE* y la librer√≠a *Plotly* para representar la proximidad sem√°ntica entre las palabras en un espacio reducido.
- **Obtener Conclusiones**: Se han derivado conclusiones detalladas a partir del preprocesamiento, el entrenamiento del modelo, los resultados de las pruebas de similitud/analog√≠a y las visualizaciones de los *embeddings*.

---

## Caracter√≠sticas implementadas

- **Descarga autom√°tica de recursos *NLTK***: asegura que todos los corpus y modelos *NLTK* necesarios est√©n disponibles.
- **Preprocesamiento de texto robusto**: incluye limpieza basada en expresiones regulares para eliminar elementos estructurales (marcadores de acto/escena, etiquetas de hablantes, direcciones de escena) y pasos est√°ndar de PLN (tokenizaci√≥n, min√∫sculas, lematizaci√≥n, eliminaci√≥n de *stopwords*).
- **Entrenamiento de *Word2Vec* con seguimiento de p√©rdidas**: implementa una funci√≥n de *callback* personalizada para monitorear la p√©rdida de entrenamiento por √©poca, lo que proporciona informaci√≥n sobre la convergencia del modelo.
- **Consultas de similitud de palabras**: demuestra la funcionalidad *most_similar* para encontrar palabras sem√°nticamente cercanas a t√©rminos de inter√©s dentro del contexto de "Hamlet".
- **Aritm√©tica vectorial para analog√≠as**: intenta realizar analog√≠as vectoriales (por ejemplo, "rey" - "hombre" + "mujer" = "reina") para probar la capacidad del modelo para capturar relaciones sem√°nticas abstractas.
- **Visualizaciones interactivas *t-SNE***: genera diagramas de dispersi√≥n interactivos en 2D y 3D utilizando *Plotly*, lo que permite la exploraci√≥n visual de cl√∫steres de palabras en el espacio de *embedding*.

---

## An√°lisis y resultados

### Preprocesamiento del corpus

El *pipeline* de preprocesamiento extrajo aproximadamente 3735 oraciones efectivas de "Hamlet" para el entrenamiento de *Word2Vec*. Las observaciones clave incluyen:

- **Inclusi√≥n de elementos estructurales**: el preprocesamiento retuvo t√©rminos como *actus*, *primus*, *scoena*, *prima*, *enter*, *barnardo*, *francisco*, y abreviaturas de nombres de personajes (*fran*).
- **Ortograf√≠as arcaicas**: el modelo aprendi√≥ *embeddings* para ortograf√≠as arcaicas (ej., *vnfold*, *liue*), reflejando el vocabulario espec√≠fico del ingl√©s shakespeariano.
- **Limpieza efectiva**: la lematizaci√≥n y la eliminaci√≥n de *stopwords* mejoraron significativamente la calidad de los *tokens*.

### Entrenamiento del modelo *Word2Vec*

El modelo *Word2Vec* fue entrenado con 3735 oraciones procesadas durante 100 √©pocas. La p√©rdida por √©poca disminuy√≥ progresivamente de ~274k a ~28k, indicando un aprendizaje efectivo y la convergencia del modelo. El tama√±o del vocabulario final fue de 4145 palabras.

### Pruebas de similitud y analog√≠a

#### Palabras m√°s similares
Los resultados de *most_similar* demuestran la capacidad del modelo para capturar asociaciones sem√°nticas muy espec√≠ficas y contextualmente relevantes dentro de "Hamlet" para las palabras presentes en su vocabulario.

* Para *ophelia*, se observan asociaciones como *beautifed*, *idoll*, *orizons* y *nimph* (similitudes >0.73).
* La fuerte relaci√≥n de *queen* con *willow* y *aslant* (ambas >0.77) vincula directamente a la descripci√≥n de la muerte de Ofelia narrada por Gertrudis.
* *ghost* muestra una conexi√≥n tem√°tica con *adulterate* (0.6547).
* T√©rminos geopol√≠ticos como *denmark* y *fortinbras* se asociaron con un l√©xico militar y pol√≠tico relevante, con similitudes a menudo superiores a 0.79.
* Se identificaron asociaciones menos intuitivas para *hamlet* (*vnbrac*, *doublet*), posiblemente ruido estad√≠stico o la influencia de co-ocurrencias espor√°dicas.

#### Similitudes entre pares
La similitud coseno entre pares seleccionados cuantifica su cercan√≠a en el espacio vectorial aprendido:

* La relaci√≥n *king* - *claudius* (0.4284) indica una asociaci√≥n sem√°ntica moderada.
* Pares como *hamlet* - *ophelia* (0.2943) y *king* - *queen* (0.2157) exhiben similitudes m√°s d√©biles.
* La similitud *death* - *ghost* (0.2938) fue moderada-baja.

#### Limitaciones del vocabulario
Una limitaci√≥n cr√≠tica observada es la ausencia de t√©rminos tem√°ticos clave como *love*, *madness*, *revenge*, *skull* y *poison* en el vocabulario del modelo, probablemente debido a su frecuencia o al preprocesamiento.

#### Pruebas de analog√≠a
Las pruebas de analog√≠a (*king* - *man* + *woman* $\approx$ *queen* y *laertes* - *polonius* + *ghost* $\approx$ *hamlet*) no identificaron los t√©rminos can√≥nicos esperados. Esto se atribuye principalmente a la especificidad del corpus de "Hamlet", que es relativamente peque√±o y altamente especializado, limitando la capacidad del modelo para aprender subestructuras lineales generalizables para relaciones sem√°nticas abstractas.

### Visualizaciones *t-SNE* de *Embeddings*

#### Gr√°fico *t-SNE* 2D (ver figura en el [archivo original en Colab](https://colab.research.google.com/drive/1-3nsIWYq2D5WzH5Ume3_fTlERh8xvQrz?usp=sharing#scrollTo=leSnYEBkCsii))
La proyecci√≥n 2D muestra agrupaciones tem√°ticas, con cierto solapamiento. Una agrupaci√≥n densa de personajes prominentes (*hamlet*, *claudius*, *ghost*, *horatio*, *king*, *queen*, *gertrude*, *polonius*) resalta su prominencia en contextos comunes. Sin embargo, la bidimensionalidad impone limitaciones, con algunas palabras clave apareciendo m√°s aisladas.
![Gr√°fico t-SNE 2D](Desafio_2/Figuras/t-SNE%202D.png)

#### Gr√°fico *t-SNE* 3D (ver figura en el [archivo original en Colab](https://colab.research.google.com/drive/1-3nsIWYq2D5WzH5Ume3_fTlERh8xvQrz?usp=sharing#scrollTo=leSnYEBkCsii))
Las visualizaciones tridimensionales ofrecen una representaci√≥n m√°s rica y matizada. Los cl√∫steres observados en 2D se reafirman y organizan de manera m√°s coherente. Se revelan subestructuras no evidentes en 2D: t√©rminos como *madness*, *death*, *revenge* y *father* parecen formar un subcl√∫ster tem√°tico. Palabras que parec√≠an aisladas en 2D se integran de manera m√°s natural.
![Gr√°fico t-SNE 3D](Desafio_2/Figuras/t-SNE%203D.png)

## Conclusiones
Este desaf√≠o ha demostrado la capacidad de los *Word Embeddings* generados con *Word2Vec* para capturar las relaciones sem√°nticas dentro de un corpus literario espec√≠fico como "Hamlet". Se cumplieron los objetivos del desaf√≠o al crear *embeddings* propios, probar t√©rminos y explicar similitudes, graficar los *embeddings* y obtener conclusiones. No obstante, a pesar de las limitaciones observadas en las pruebas de analog√≠a (atribuibles a la especificidad del corpus), la riqueza de las relaciones contextuales capturadas justifica el enfoque de *Word Embeddings* para la exploraci√≥n sem√°ntica de obras literarias espec√≠ficas.

---
---

## Desaf√≠o 3 ‚Äì Modelado de lenguaje a nivel de caracteres
---

### Consigna
- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.
- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validaci√≥n.
- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.
- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determin√≠stico y estoc√°stico. En este √∫ltimo caso observar el efecto de la temperatura en la generaci√≥n de secuencias.

### Sugerencias
- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validaci√≥n para finalizar el entrenamiento. Para ello se provee un callback.
- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.
- *rmsprop* es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.

---

### Objetivo
Entrenar y comparar modelos de lenguaje *many-to-many* (**SimpleRNN**, **GRU** y **LSTM**) para predecir el siguiente car√°cter en una secuencia y generar nuevo texto, evaluando.

----

### Metodolog√≠a propuesta

---
#### 1. Selecci√≥n del corpus
Se seleccion√≥ el **[ArXiv Scientific Research Papers Dataset](https://www.kaggle.com/datasets/sumitm004/arxiv-scientific-research-papers-dataset)** de Kaggle, un corpus textual representativo del dominio de investigaci√≥n en inteligencia artificial, aprendizaje autom√°tico, inform√°tica y matem√°ticas.  
Para el entrenamiento, se compilaron en un √∫nico texto los 25 res√∫menes m√°s extensos de las cuatro categor√≠as con m√°s art√≠culos.  
Este enfoque asegur√≥ que el corpus fuera representativo y contuviera la variabilidad l√©xica y sint√°ctica necesaria para una buena generalizaci√≥n del modelo.

**Figura 1.** Top 15 categor√≠as m√°s frecuentes.  
![Top 15 categor√≠as](./Desafio_3/figures/top15_categories_hist.png)

Como se muestra en la figura 1, las categor√≠as dominantes en el corpus seleccionado son **Machine Learning**, **Computer Vision and Pattern Recognition**, **Computation and Language (Natural Language Processing)** y **Artificial Intelligence**. La figura 2 muestra que el corpus final tiene una distribuci√≥n uniforme de la cantidad de palabras entre las cuatro categor√≠as seleccionadas, lo que ayuda a evitar un sesgo significativo del modelo hacia una sola disciplina.

**Figura 2.** Cantidad de palabras por categor√≠a.  
![Palabras por categor√≠a](./Desafio_3/figures/top_categories_words_sum.png)

---

#### 2. Preprocesamiento del texto
El corpus fue normalizado y tokenizado car√°cter a car√°cter:

1. Conversi√≥n a min√∫sculas.  
2. Mapeo de cada car√°cter a un √≠ndice (`char2idx`) y su inverso (`idx2char`), guardados como archivos JSON.  
3. Definici√≥n de secuencias de contexto de **100 caracteres** (`max_context_size`).  
4. Generaci√≥n de ejemplos de entrenamiento con ventana deslizante (*stride = 1*).  
5. Divisi√≥n en:
   - **Entrenamiento:** 90% inicial.
   - **Validaci√≥n:** 10% final, en bloques sin solapamiento.

---

#### 3. Dise√±o del modelo

##### 3.1. Modelos
Se implementaron tres variantes de redes recurrentes (`./Desafio_3/src/architectures.py`):

- **SimpleRNN:** entrada *one-hot*, capa recurrente `SimpleRNN` y capa `Dense`.  
- **GRU:** capa `Embedding`, dos capas `GRU` y salida `Dense`.  
- **LSTM:** capa `Embedding`, dos capas `LSTM` y salida `Dense`.

**Configuraci√≥n com√∫n:**
- Optimizador: *RMSprop* (lr = 0.001).  
- P√©rdida: *Categorical Crossentropy*.  
- M√©trica adicional: *Perplejidad*.

##### 3.2. Callbacks
Se emple√≥ un *callback* personalizado (`./Desafio_3/src/callbacks.py`) para:

- **Perplejidad:** calculada al final de cada √©poca sobre validaci√≥n:\
$$\mathrm{PPL}(X)=\exp\left(-\frac{1}{t}\sum_{i=1}^{t}\log p_{\theta}(w_i \mid w_{<i})\right)$$

- **Early Stopping:** con `patience = 3`.  
- **Guardado autom√°tico:** del mejor modelo en `models/`.

---

#### 4. Entrenamiento

**Figura 3.** Comparaci√≥n de modelos durante el entrenamiento.  
![Comparaci√≥n de modelos](./Desafio_3/figures/model_comparison.png)

- **SimpleRNN:** peor rendimiento, alta perplejidad y limitaciones en dependencias largas.  
- **GRU:** mejor rendimiento general, menor perplejidad en validaci√≥n.  
- **LSTM:** buen rendimiento, pero con *overfitting* a partir de la √©poca 10.

---

#### 5. Generaci√≥n de texto
Se utiliz√≥ `./src/text_generator.py` para generar texto desde frases iniciales (*prompts*) como:

- `recurrent neural network`  
- `convolutional neural network`  
- `future researchs should`

**Estrategias**

##### Greedy Search (*temp = 0*)
Texto repetitivo y predecible.

**Ejemplo (GRU/LSTM):**
```
of the probability of the probability
```

**Ejemplo (SimpleRNN):**
```
to the the the the
```

##### Beam Search Estoc√°stico
**Temp = 0.5:** m√°s variedad pero a√∫n con repeticiones.  
Ejemplo (GRU - future researchs should...):
```
future researchs should of the problem of the results in the problems of the problem and the computation...
```

**Temp = 1.5:** mayor creatividad; SimpleRNN incoherente, GRU y LSTM equilibrados.  
Ejemplo (GRU - future researchs should...):
```
future researchs should a related and dependent the clearning computer and the frameworks...
```

---

#### 6. Conclusiones
- **GRU** y **LSTM** superan claramente a **SimpleRNN** en la gesti√≥n de dependencias largas.  
- La mejor combinaci√≥n fue **GRU + Beam Search Estoc√°stico + Temp = 1.5**, logrando un balance entre coherencia y creatividad.  
- El modelado car√°cter a car√°cter presenta limitaciones para generar texto coherente en este dominio, pero es √∫til para evaluar el impacto de arquitectura y estrategia de decodificaci√≥n.

---

## Desaf√≠o 4 - chatbot
### Modelo Seq2Seq (*Keras*) con un solo vocabulario y embedding compartido

Este proyecto consisti√≥ en entrenar un modelo encoder‚Äìdecoder (*seq2seq*) basado en LSTM para generar respuestas en ingl√©s a partir de pares de di√°logo. La principal decisi√≥n de dise√±o fue utilizar un √∫nico tokenizador y un solo vocabulario tanto para las entradas como para las salidas, junto con una √∫nica capa de *embedding* compartida entre encoder y decoder. Esta estrategia simplific√≥ el pipeline, evit√≥ desalineaciones en los √≠ndices y redujo significativamente el consumo de memoria.

---

### Objetivo
* Desarrollar un sistema de di√°logo sencillo en ingl√©s, trabajando con pares pregunta ‚Üí respuesta.
* Utilizar un √∫nico vocabulario (un solo Tokenizer) y una √∫nica matriz de embeddings (*GloVe* o *fastText*) reutilizada en ambas partes del modelo.
* Realizar la inferencia paso a paso, incorporando tokens especiales de inicio y fin de secuencia

---

### 1. Datos

* El dataset estuvo formado por conversaciones; de cada l√≠nea se extrajeron parejas consecutivas (entrada, salida).
* Se descartaron pares demasiado largos para evitar explotar memoria y estabilizar el entrenamiento (longitudes m√°ximas t√≠picas: 10‚Äì30 tokens).
* Se agregaron marcadores:
   * `<sos>` (start-of-sequence) al inicio de la salida para el decoder input.
   * `<eos>` (end-of-sequence) al final de la salida para el decoder target.

Limpieza recomendada
Se usaron min√∫sculas, normalizaci√≥n b√°sica de contracciones en ingl√©s, y filtrado de s√≠mbolos para dejar solo caracteres alfanum√©ricos/espacios. Fue importante reasignar los reemplazos (evitar funciones ‚Äúno in-place‚Äù).

---

### 2. Vocabulario y tokenizaci√≥n

* Se us√≥ un solo tokenizador entrenado con la uni√≥n de:

¬† * entradas,
¬† * salidas con `<eos>`,
¬† * salidas de entrada del decoder con `<sos>`.
* El tama√±o del vocabulario se recort√≥ a un m√°ximo (p. ej., 8 000) y se reserv√≥ el √≠ndice 0 para padding.
* `<sos>` y `<eos>` debieron existir en el vocabulario y tener √≠ndices > 0.
* Todas las secuencias fueron paddeadas a longitudes fijas separadas: `max_input_len` para el encoder y `max_out_len` para el decoder.

---

### 3. Embeddings

* Se emple√≥ un √∫nico conjunto de embeddings en ingl√©s (*fastText*).
* Se construy√≥ una sola matriz de tama√±o *(vocab, dim)* usando el mismo diccionario del tokenizador.
* Detalle cr√≠tico: cuando se consult√≥ el embedding de una palabra, se tuvo que tratar la palabra como unidad l√©xica, no como lista de caracteres. (En t√©rminos pr√°cticos: la funci√≥n que obtuvo embeddings tuvo que recibir un conjunto/lista de palabras, no un string suelto).
* Las palabras fuera del vocabulario de los embeddings quedaron con vector nulo; convino monitorear la cobertura (proporci√≥n de palabras con vector no nulo).

Elecci√≥n del embedding
* fastText (p. ej., wiki-news-300d): ofreci√≥ mejor cobertura por subpalabras, a costa de mayor tama√±o.
`class FasttextEmbeddings(WordsEmbeddings):
¬† WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'
¬† PKL_PATH = 'fasttext.pkl'
¬† N_FEATURES = 300
¬† WORD_MAX_SIZE = 60`

---

### 4. Arquitectura del modelo

* Embedding compartido (no entrenable, con m√°scara de padding activada): una sola capa que transform√≥ IDs de tokens en vectores, usada tanto por encoder como por decoder.
* Encoder: LSTM con 128 unidades (configurable), con `dropout` y `recurrent_dropout` t√≠picamente en 0.2.
* Decoder: otra LSTM de 128 unidades que recibi√≥ el estado final del encoder. Produjo una secuencia de logits que se proyect√≥ con una capa densa al tama√±o del vocabulario compartido.
* Funci√≥n de p√©rdida: entrop√≠a cruzada categ√≥rica sobre la salida del decoder (one-hot o soft labels).
* M√©trica: `accuracy` a nivel de token (√∫til para seguimiento; no siempre correlacion√≥ con calidad ling√º√≠stica).
![Diagrama](./Desafio_4/images/model_plot.png)

---

### 5. Entrenamiento

* Se entren√≥ con *teacher forcing*: el decoder vio la secuencia de salida ‚Äúreal‚Äù desplazada por `<sos>`.
* Se realiz√≥ una partici√≥n 80:20
* √âpocas t√≠picas: 100
¬†¬†
#### 5.1. Monitoreo
* Las curvas del accuracy y loss mostraron lo que pareci√≥ ser un overfitting, sin embargo, el accuracy no fue una buena m√©trica para lenguajes de procesamiento de lenguaje natural.
![Curvas](./Desafio_4/images/training_curves.png)

---

#### 5.2. Inferencia (decodificaci√≥n)

* Se construy√≥ un encoder de inferencia que, dado el input paddeado, devolvi√≥ los estados ocultos iniciales del decoder.
* El decoder de inferencia funcion√≥ token a token:

¬† 1. Se inici√≥ con `<sos>`.
¬† 2. En cada paso se tom√≥ el token previo, se lo pas√≥ por la misma capa de Embedding compartida, se propag√≥ en el LSTM junto con los estados, y se obtuvo una distribuci√≥n sobre el vocabulario.
¬† 3. Se seleccion√≥ el siguiente token (*greedy*).
¬† 4. Se detuvo al predecir `<eos>` o alcanzar la longitud m√°xima.

> Clave: el decoder de inferencia reutiliz√≥ exactamente las mismas capas y pesos del entrenamiento (Embedding, LSTM y Dense). No se crearon capas nuevas ‚Äúen blanco‚Äù.

Ejemplos de inferencia en preguntas elaboradas por el usario (üßîüèΩ‚Äç‚ôÇÔ∏è) que responde el *chatbot*(ü§ñ):

üßîüèΩ‚Äç‚ôÇÔ∏è *"What do you do for a living?"*

ü§ñ *"i am a student"*

-
üßîüèΩ‚Äç‚ôÇÔ∏è *"Do you read?"*

ü§ñ *"yes"*

-
üßîüèΩ‚Äç‚ôÇÔ∏è *"Do you have any pet?"*

ü§ñ *"yes i have a tiger"*

-
üßîüèΩ‚Äç‚ôÇÔ∏è *"Where are you from?"*
ü§ñ *"i am from the united states"*


---

### Conclusiones

Un solo vocabulario y un solo Embedding simplificaron el entrenamiento y la inferencia, evitando errores de √≠ndices y reduciendo la memoria. Con limpieza adecuada, cobertura de embeddings razonable y un pipeline de inferencia que reutiliz√≥ las mismas capas entrenadas, el sistema produjo respuestas muy coherentes para di√°logos simples en ingl√©s.
