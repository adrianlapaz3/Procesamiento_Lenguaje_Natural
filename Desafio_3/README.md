Perfecto, aqu√≠ te dejo un **README.md m√°s detallado** que incluye las im√°genes y un contexto m√°s completo para tu repositorio:

---

# üìÑ Character-Level Language Model con RNN, LSTM y GRU

Este proyecto implementa y compara modelos de lenguaje car√°cter a car√°cter utilizando tres arquitecturas recurrentes: **SimpleRNN**, **LSTM** y **GRU**. El objetivo es predecir el pr√≥ximo car√°cter en una secuencia de texto y generar texto nuevo empleando estrategias como *greedy search* y *beam search* (determinista y estoc√°stico).

---

## üìÇ Estructura del repositorio

* **`4_modelo_lenguaje_char.ipynb`** ‚Äì Notebook principal con preprocesamiento, entrenamiento, evaluaci√≥n y generaci√≥n de texto.
* **`architectures.py`** ‚Äì Definici√≥n de las clases de modelos:

  * `SimpleRNNModel`
  * `LSTMModel`
  * `GRUModel`
* **`callbacks.py`** ‚Äì *Callback* personalizado `PplCallback` para calcular perplejidad, guardar el mejor modelo y detener el entrenamiento si no hay mejoras.
* **`text_generator.py`** ‚Äì Funciones para generaci√≥n de texto y *beam search*.
* **`model_comparison.png`**, **`top15_categories_hist.png`**, **`top_categories_words_sum.png`** ‚Äì Gr√°ficos de resultados y an√°lisis exploratorio del corpus.

---

## üìä An√°lisis exploratorio del corpus

El corpus seleccionado proviene de art√≠culos cient√≠ficos (dataset de arXiv), filtrando las categor√≠as m√°s relevantes y las palabras m√°s frecuentes en res√∫menes.

**Top 15 categor√≠as m√°s frecuentes**
![Top 15 categor√≠as](top15_categories_hist.png)

**Palabras totales por categor√≠a (top 100 res√∫menes seleccionados)**
![Palabras por categor√≠a](top_categories_words_sum.png)

---

## üèóÔ∏è Modelos implementados

Las arquitecturas se entrenaron para aprender representaciones car√°cter a car√°cter y generar texto.

**Comparaci√≥n de Accuracy, Loss y Perplejidad**
![Comparaci√≥n de modelos](model_comparison.png)

**Principales observaciones:**

* **GRU** obtuvo el mejor equilibrio entre velocidad de convergencia y perplejidad final.
* **LSTM** logr√≥ buena capacidad de generalizaci√≥n, aunque con m√°s coste computacional.
* **SimpleRNN** present√≥ limitaciones en dependencias largas y mayor perplejidad.

---

## üöÄ Estrategias de generaci√≥n

Se implementaron tres m√©todos para generar texto:

1. **Greedy Search** ‚Äì Selecciona siempre el car√°cter m√°s probable.
2. **Beam Search Determinista** ‚Äì Explora varias trayectorias y selecciona las de mayor probabilidad acumulada.
3. **Beam Search Estoc√°stico** ‚Äì Introduce aleatoriedad controlada con un par√°metro de temperatura, aumentando la diversidad.

---

## ‚öôÔ∏è Requisitos

```bash
tensorflow>=2.x
numpy
scipy
matplotlib
```

---

## ‚ñ∂Ô∏è Ejecuci√≥n

1. Clonar el repositorio:

```bash
git clone https://github.com/usuario/char-level-language-model.git
cd char-level-language-model
```

2. Entrenar el modelo ejecutando el notebook:

```bash
jupyter notebook 4_modelo_lenguaje_char.ipynb
```

3. Generar texto usando las funciones en `text_generator.py`.

---

## üìå Conclusiones

* LSTM y GRU superan a SimpleRNN en tareas con dependencias largas.
* *Beam Search* estoc√°stico con temperatura moderada (\~1.0‚Äì1.2) equilibra coherencia y creatividad.
* Un callback de perplejidad permite evaluar de forma m√°s precisa la calidad del modelo.

---

Si quer√©s, puedo armarte tambi√©n **un ejemplo en el README con c√≥digo para generar texto usando tu `beam_search`**, de forma que cualquiera pueda probarlo r√°pido. ¬øQuieres que lo incluya?
