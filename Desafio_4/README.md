# Modelo Seq2Seq (*Keras*) con un solo vocabulario y embedding compartido

Este proyecto consistiÃ³ en entrenar un modelo encoderâ€“decoder (*seq2seq*) basado en LSTM para generar respuestas en inglÃ©s a partir de pares de diÃ¡logo. La principal decisiÃ³n de diseÃ±o fue utilizar un Ãºnico tokenizador y un solo vocabulario tanto para las entradas como para las salidas, junto con una Ãºnica capa de *embedding* compartida entre encoder y decoder. Esta estrategia simplificÃ³ el pipeline, evitÃ³ desalineaciones en los Ã­ndices y redujo significativamente el consumo de memoria.

---

## Objetivo
* Desarrollar un sistema de diÃ¡logo sencillo en inglÃ©s, trabajando con pares pregunta â†’ respuesta.
* Utilizar un Ãºnico vocabulario (un solo Tokenizer) y una Ãºnica matriz de embeddings (*GloVe* o *fastText*) reutilizada en ambas partes del modelo.
* Realizar la inferencia paso a paso, incorporando tokens especiales de inicio y fin de secuencia

---

## 1. Datos

* El dataset estuvo formado por conversaciones; de cada lÃ­nea se extrajeron parejas consecutivas (entrada, salida).
* Se descartaron pares demasiado largos para evitar explotar memoria y estabilizar el entrenamiento (longitudes mÃ¡ximas tÃ­picas: 10â€“30 tokens).
* Se agregaron marcadores:

Â  * `<sos>` (start-of-sequence) al inicio de la salida *para el decoder input*.
Â  * `<eos>` (end-of-sequence) al final de la salida *para el decoder target*.

Limpieza recomendada
Se usaron minÃºsculas, normalizaciÃ³n bÃ¡sica de contracciones en inglÃ©s, y filtrado de sÃ­mbolos para dejar solo caracteres alfanumÃ©ricos/espacios. Fue importante reasignar los reemplazos (evitar funciones â€œno in-placeâ€).

---

## 2. Vocabulario y tokenizaciÃ³n

* Se usÃ³ un solo tokenizador entrenado con la uniÃ³n de:

Â  * entradas,
Â  * salidas con `<eos>`,
Â  * salidas de entrada del decoder con `<sos>`.
* El tamaÃ±o del vocabulario se recortÃ³ a un mÃ¡ximo (p. ej., 8 000) y se reservÃ³ el Ã­ndice 0 para padding.
* `<sos>` y `<eos>` debieron existir en el vocabulario y tener Ã­ndices > 0.
* Todas las secuencias fueron paddeadas a longitudes fijas separadas: `max_input_len` para el encoder y `max_out_len` para el decoder.

---

## 3. Embeddings

* Se empleÃ³ un Ãºnico conjunto de embeddings en inglÃ©s (*fastText*).
* Se construyÃ³ una sola matriz de tamaÃ±o *(vocab, dim)* usando el mismo diccionario del tokenizador.
* Detalle crÃ­tico: cuando se consultÃ³ el embedding de una palabra, se tuvo que tratar la palabra como unidad lÃ©xica, no como lista de caracteres. (En tÃ©rminos prÃ¡cticos: la funciÃ³n que obtuvo embeddings tuvo que recibir un conjunto/lista de palabras, no un string suelto).
* Las palabras fuera del vocabulario de los embeddings quedaron con vector nulo; convino monitorear la cobertura (proporciÃ³n de palabras con vector no nulo).

ElecciÃ³n del embedding
* fastText (p. ej., wiki-news-300d): ofreciÃ³ mejor cobertura por subpalabras, a costa de mayor tamaÃ±o.
`class FasttextEmbeddings(WordsEmbeddings):
Â  WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'
Â  PKL_PATH = 'fasttext.pkl'
Â  N_FEATURES = 300
Â  WORD_MAX_SIZE = 60`

---

## 4. Arquitectura del modelo

* Embedding compartido (no entrenable, con mÃ¡scara de padding activada): una sola capa que transformÃ³ IDs de tokens en vectores, usada tanto por encoder como por decoder.
* Encoder: LSTM con 128 unidades (configurable), con `dropout` y `recurrent_dropout` tÃ­picamente en 0.2.
* Decoder: otra LSTM de 128 unidades que recibiÃ³ el estado final del encoder. Produjo una secuencia de logits que se proyectÃ³ con una capa densa al tamaÃ±o del vocabulario compartido.
* FunciÃ³n de pÃ©rdida: entropÃ­a cruzada categÃ³rica sobre la salida del decoder (one-hot o soft labels).
* MÃ©trica: `accuracy` a nivel de token (Ãºtil para seguimiento; no siempre correlacionÃ³ con calidad lingÃ¼Ã­stica).
![Diagrama](./images/model_plot.png)

---

## 5. Entrenamiento

* Se entrenÃ³ con *teacher forcing*: el decoder vio la secuencia de salida â€œrealâ€ desplazada por `<sos>`.
* Se realizÃ³ una particiÃ³n 80:20
* Ã‰pocas tÃ­picas: 100
Â Â 
### 5.1. Monitoreo
* Las curvas del accuracy y loss mostraron lo que pareciÃ³ ser un overfitting, sin embargo, el accuracy no fue una buena mÃ©trica para lenguajes de procesamiento de lenguaje natural.
![Curvas](./images/training_curves.png)

---

### 5.2. Inferencia (decodificaciÃ³n)

* Se construyÃ³ un encoder de inferencia que, dado el input paddeado, devolviÃ³ los estados ocultos iniciales del decoder.
* El decoder de inferencia funcionÃ³ token a token:

Â  1. Se iniciÃ³ con `<sos>`.
Â  2. En cada paso se tomÃ³ el token previo, se lo pasÃ³ por la misma capa de Embedding compartida, se propagÃ³ en el LSTM junto con los estados, y se obtuvo una distribuciÃ³n sobre el vocabulario.
Â  3. Se seleccionÃ³ el siguiente token (greedy).
Â  4. Se detuvo al predecir `<eos>` o alcanzar la longitud mÃ¡xima.

> Clave: el decoder de inferencia reutilizÃ³ exactamente las mismas capas y pesos del entrenamiento (Embedding, LSTM y Dense). No se crearon capas nuevas â€œen blancoâ€.

Ejemplos de inferencia:
* *Input:*  ğŸ§”ğŸ½â€â™‚ï¸  what do you do for a living
* *Output:* ğŸ¤– i am a student
  
* *Input:*  ğŸ§”ğŸ½â€â™‚ï¸  Do you read?
* *Output:* ğŸ¤– yes
  
* *Input:*  ğŸ§”ğŸ½â€â™‚ï¸ Do you have any pet?
* *Output:* ğŸ¤– yes i have a tiger
  
* *Input:*  ğŸ§”ğŸ½â€â™‚ï¸ Where are you from?
* *Output:* ğŸ¤– i am from the united states

---

### Conclusiones

Un solo vocabulario y un solo Embedding simplificaron el entrenamiento y la inferencia, evitando errores de Ã­ndices y reduciendo la memoria. La consistencia entre tokenizador, matriz de embeddings y capas (mismos tamaÃ±os e Ã­ndices) fue la clave para resultados estables. Con limpieza adecuada, cobertura de embeddings razonable y un pipeline de inferencia que reutilizÃ³ las mismas capas entrenadas, el sistema produjo respuestas muy coherentes para diÃ¡logos simples en inglÃ©s.
